{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:30:19.660356Z",
     "start_time": "2019-08-01T14:30:18.694776Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import time \n",
    "import datetime \n",
    "import logging\n",
    "import json \n",
    "import random\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from collections import Counter\n",
    "import gensim\n",
    "import numpy as np \n",
    "import tensorflow as tf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义参数配置类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:45:53.522955Z",
     "start_time": "2019-08-01T14:45:53.516289Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, path=None):\n",
    "        super().__init__()\n",
    "        ## 定义训练参数\n",
    "        self['num_epochs'] = 5 \n",
    "        self['evaluateEvery'] = 100 \n",
    "        self['checkpointEvery'] = 100 \n",
    "        self['learningRate'] = 0.001 \n",
    "        \n",
    "        ## 定义模型参数\n",
    "        self['embeddingSize'] = 200 \n",
    "        self['numFilters'] = 128 \n",
    "        self['filterSizes'] = [2, 3, 4, 5]\n",
    "        self['dropoutProb'] = 0.5 \n",
    "        self['l2RegLambda'] = 0.0 \n",
    "        \n",
    "        ## 定义基础参数\n",
    "        self['sequenceLength'] = 200 \n",
    "        self['batch_size'] = 64 \n",
    "        self['dataSource'] = path\n",
    "        self['stopWordSource'] = \"../data/english\"\n",
    "        self['numClasses'] = 1  \n",
    "        self['train_size'] = 0.8   # 训练集和测试集比例\n",
    "        \n",
    "        ## 保存模型参数\n",
    "        self['checkpoint_dir'] = \"../model/textCNN/checkpoint\"\n",
    "        self['summary_dir'] = \"../model/textCNN/summary\"\n",
    "        self['max_to_keep'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:30:19.791442Z",
     "start_time": "2019-08-01T14:30:19.666857Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义数据预处理类\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config['dataSource']\n",
    "        self._stopWordSource = config['stopWordSource']\n",
    "        \n",
    "        self._sequenceLength = config['sequenceLength'] # 设置序列的输入藏毒\n",
    "        self._embeddingSize = config['embeddingSize']\n",
    "        self._batchSize = config['batch_size']\n",
    "        self._trainRate = config['train_size']\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding = None\n",
    "        self.labelList = []\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        '''\n",
    "        从csv文件中读取数据集\n",
    "        '''\n",
    "        df = pd.read_csv(filePath)\n",
    "        if self.config['numClasses'] == 1:          \n",
    "            if \"sentiment\" in df.columns:\n",
    "                labels = df[\"sentiment\"].tolist()\n",
    "            if \"emotion\" in df.columns:\n",
    "                labels = df[\"emotion\"].tolist()\n",
    "        \n",
    "        elif self.config['numClasses'] > 1: \n",
    "            labels = df[\"rate\"].tolist()\n",
    "        \n",
    "        review = df['review'].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "        \n",
    "        return reviews, labels\n",
    "    \n",
    "    def _laeblToIndex(self, labels, label2idx):\n",
    "        '''\n",
    "        将标签转换为索引表示\n",
    "        '''\n",
    "        labelIds = [label2idx[label] for label in labels]\n",
    "        return labelIds\n",
    "    \n",
    "    def _wordToIndex(self, reviews, word2idx):\n",
    "        '''\n",
    "        将词转换为索引表示\n",
    "        '''\n",
    "        reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
    "        return reviewIds\n",
    "    \n",
    "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
    "        '''\n",
    "        生成训练集和验证集\n",
    "        '''\n",
    "        reviews = []\n",
    "        for review in x: \n",
    "            if len(review) >= self._sequenceLength:\n",
    "                reviews.append(review[:self._sequenceLength])\n",
    "            else:\n",
    "                reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
    "        \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(y[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(y[trainIndex:], dtype=\"float32\")\n",
    "        \n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "    \n",
    "    def _genVocabulary(self, reviews, labels, path, prefix=\"\"):\n",
    "        '''\n",
    "        生成向量和词汇-索引映射字典\n",
    "        '''\n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        wordCount = Counter(subWords)  # 统计各个词的词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words, path)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        #print(len(vocab), vocab[10])\n",
    "        word2idx = dict(zip(vocab, range(len(vocab))))\n",
    "        \n",
    "        uniqueLabel = list(set(labels))\n",
    "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
    "        self.labelList = list(range(len(uniqueLabel)))\n",
    "        \n",
    "        # 将词汇表-索引映射表保存为json数据，之后inference时直接加载处理数据\n",
    "        save_path = \"../data/wordJson\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        with open(os.path.join(save_path, prefix+\"word2idx.json\"), \"w\", encoding=\"utf8\") as f: \n",
    "            json.dump(word2idx, f)\n",
    "        \n",
    "        with open(os.path.join(save_path, prefix+\"label2idx.json\"), \"w\", encoding=\"utf8\") as f: \n",
    "            json.dump(label2idx, f)\n",
    "        \n",
    "        return word2idx, label2idx\n",
    "    \n",
    "    def _getWordEmbedding(self, words, path):\n",
    "        '''\n",
    "        按照数据集中的单词去除训练好的词向量\n",
    "        '''\n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(path, \"wordvector.bin\"),\n",
    "                                                                 binary=True)\n",
    "        \n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加\"pad\"和\"UNK\"\n",
    "        vocab.append(\"PAD\")\n",
    "        vocab.append(\"UNK\")\n",
    "        \n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))  # 表示对\"PAD\"用全0向量表示\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))  # 对\"UNK\"用随机向量表示\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopword(self, stopWordPath):\n",
    "        '''\n",
    "        读取停用词\n",
    "        '''\n",
    "        with open(stopWordPath, \"r\") as f: \n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 转换成字典的形式，使用hash查找效率更高\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self, path, prefix=\"\"):\n",
    "        '''\n",
    "        初始化训练集和验证集 \n",
    "        prefix: 表示生成单词到索引的文件的前缀\n",
    "        path: 表示wordvector文件的位置\n",
    "        '''\n",
    "        # 初始化停用词\n",
    "        self._readStopword(self._stopWordSource)\n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        word2idx, label2idx = self._genVocabulary(reviews, labels, path, prefix)\n",
    "        # 将标签和句子数值化\n",
    "        labelIds = self._laeblToIndex(labels, label2idx)\n",
    "        reviewsIds = self._wordToIndex(reviews, word2idx)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviewsIds, \n",
    "                                                                                  labelIds,\n",
    "                                                                                  word2idx, \n",
    "                                                                                  self._trainRate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义数据的迭代类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:30:19.894951Z",
     "start_time": "2019-08-01T14:30:19.794732Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x \n",
    "        self.y = y\n",
    "        self.length = len(y)\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        '''\n",
    "        生成每一个batch的数据集\n",
    "        '''\n",
    "        idx = np.random.choice(self.length, batch_size)\n",
    "        yield self.x[idx], self.y[idx]\n",
    "        \n",
    "    def iter_all(self, batch_size):\n",
    "        '''\n",
    "        按照batch迭代所有数据 \n",
    "        '''        \n",
    "        numBatches = self.length // batch_size\n",
    "        for i in range(numBatches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            batchX = np.array(self.x[start:end], dtype='int64')\n",
    "            batchY = np.array(self.y[start:end], dtype=\"float32\")\n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义每个模型都要继承的基类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:30:20.060715Z",
     "start_time": "2019-08-01T14:30:20.055771Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    def __init__(self, config):\n",
    "        self.config = config \n",
    "        self.init_global_step()\n",
    "        self.init_cur_epoch()\n",
    "        \n",
    "    def save(self, sess):\n",
    "        print(\"Saving model...\")\n",
    "        self.saver.save(sess, self.config['checkpoint_dir']+\"/my_model\", self.global_step_tensor)\n",
    "        print(\"Model saved\")\n",
    "        \n",
    "    def load(self, sess):\n",
    "        ## 获取最近的chekpoint\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(self.config['checkpoint_dir'])\n",
    "        if latest_checkpoint:\n",
    "            print(\"Loading model checkpoint {} ... \\n\".format(latest_checkpoint))\n",
    "            self.saver.restore(sess, latest_checkpoint)\n",
    "            print(\"Model loaded\")\n",
    "    \n",
    "    # 表示每执行一个epoch，对应的变量+1\n",
    "    def init_cur_epoch(self):\n",
    "        with tf.variable_scope(\"cur_epoch\"):\n",
    "            self.cur_epoch_tensor = tf.Variable(0, trainable=False, name=\"cur_epoch\")\n",
    "            self.increment_cur_epoch_tensor = tf.assign(self.cur_epoch_tensor, self.cur_epoch_tensor+1)\n",
    "    \n",
    "    def init_global_step(self):\n",
    "        # 表示当前模型一共迭代的step\n",
    "        ## 每次执行都需要放到trainer里面\n",
    "        with tf.variable_scope(\"global_step\"):\n",
    "            self.global_step_tensor = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    \n",
    "    def init_saver(self):\n",
    "        # 通常只需要在子类中拷贝下面的语句即可\n",
    "        # self.saver = tf.train.Saver(max_to_keep=self.config['max_to_keep'])\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def build_model(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:30:20.812071Z",
     "start_time": "2019-08-01T14:30:20.807939Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseTrain:\n",
    "    def __init__(self, sess, model, data, config, logger):\n",
    "        self.model = model \n",
    "        self.logger = logger\n",
    "        self.config = config\n",
    "        self.data = data \n",
    "        self.sess = sess\n",
    "        self.init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        self.sess.run(self.init)\n",
    "        \n",
    "    def train_all(self):\n",
    "        for cur_epoch in range(self.model.cur_epoch_tensor.eval(self.sess), self.config['num_epochs']+1, 1):\n",
    "            print(f\"\\n当前正处于第{cur_epoch+1}次迭代\")\n",
    "            self.train_epoch()\n",
    "            ## 将对应的epoch+1\n",
    "            self.sess.run(self.model.increment_cur_epoch_tensor)\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        '''\n",
    "        实现一个epoch训练的代码\n",
    "        - 在config规定的迭代次数上迭代，调用train_step\n",
    "        - 添加summary\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def train_step(self):\n",
    "        '''\n",
    "        实现单步训练的逻辑代码\n",
    "        '''\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义记录训练过程中一些信息的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:30:21.136388Z",
     "start_time": "2019-08-01T14:30:21.129372Z"
    }
   },
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, sess, config):\n",
    "        self.sess = sess \n",
    "        self.config = config \n",
    "        self.summary_placeholders = {}\n",
    "        self.summary_ops = {}\n",
    "        self.train_sumary_writer = tf.summary.FileWriter(os.path.join(self.config['summary_dir'], \"train\"),\n",
    "                                                        self.sess.graph)\n",
    "        self.test_summary_writer = tf.summary.FileWriter(os.path.join(self.config['summary_dir'], \"test\"))\n",
    "        \n",
    "    # 保存scalars和images\n",
    "    def summarize(self, step, summarizer=\"train\", scope=\"\", summaries_dict=None):\n",
    "        '''\n",
    "        step: 表示summary的时间步\n",
    "        summarizer: 表示使用 train 还是 test\n",
    "        scope: 表示变量空间名 \n",
    "        summaries_dict: 表示要summaries的值，格式是(tag, value)\n",
    "        '''\n",
    "        summary_writer = self.train_sumary_writer if summarizer == \"train\" else self.test_summary_writer\n",
    "        with tf.variable_scope(scope):\n",
    "            if summaries_dict is not None:\n",
    "                summary_list = []\n",
    "                for tag, value in summaries_dict.items():\n",
    "                    if tag not in self.summary_ops:\n",
    "                        if len(value.shape) <= 1:\n",
    "                            self.summary_placeholders[tag] = tf.placeholder(tf.float32,shape=value.shape, name=tag)\n",
    "                        else:\n",
    "                            self.summary_placeholders[tag] = tf.placeholder(\"float32\", \n",
    "                                                                            [None]+list(value.shape[1:]), \n",
    "                                                                           name=tag)                                     \n",
    "                        if len(value.shape) <= 1:\n",
    "                            ## 添加标量\n",
    "                            self.summary_ops[tag] = tf.summary.scalar(tag, self.summary_placeholders[tag])\n",
    "                        else:\n",
    "                            ## 添加为图片\n",
    "                            self.summary_ops[tag] = tf.summary.image(tag, self.summary_placeholders[tag])\n",
    "\n",
    "                    summary_list.append(self.sess.run(self.summary_ops[tag], \n",
    "                                                      {self.summary_placeholders[tag]: value}))\n",
    "                for summary in summary_list:\n",
    "                    summary_writer.add_summary(summary, step)\n",
    "                summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:30:22.381357Z",
     "start_time": "2019-08-01T14:30:22.366252Z"
    }
   },
   "outputs": [],
   "source": [
    "class Metric(object):\n",
    "    def __init__(self, pred_y, true_y, labels=None):\n",
    "        self.pred_y = pred_y\n",
    "        self.true_y = true_y\n",
    "        self.labels = labels\n",
    "    \n",
    "    @classmethod\n",
    "    def mean(cls, item: list) -> float:\n",
    "        '''\n",
    "        定义计算列表元素均值的函数\n",
    "        '''\n",
    "        res = sum(item) / len(item) if len(item) > 0 else 0\n",
    "        return round(res, 5) \n",
    "    \n",
    "    def accuracy(self):\n",
    "        '''\n",
    "        计算二类和多类的准确率\n",
    "        '''\n",
    "        p = self.pred_y\n",
    "        t = self.true_y\n",
    "        if isinstance(p[0], list):\n",
    "            p = [item[0] for item in p]\n",
    "        corr = 0 \n",
    "        for i in range(len(p)):\n",
    "            if p[i] == t[i]:\n",
    "                corr += 1 \n",
    "        acc = corr / len(p) if len(p) > 0 else 0 \n",
    "        return round(acc, 5)\n",
    "\n",
    "    def binary_precision(self, positive=1):\n",
    "        '''\n",
    "        二类精确率的计算 \n",
    "        '''\n",
    "        p = self.pred_y\n",
    "        t = self.true_y\n",
    "        if isinstance(p[0], list):\n",
    "            p = [item[0] for item in p]\n",
    "        corr = 0 \n",
    "        pred_corr = 0 \n",
    "        for i in range(len(p)):\n",
    "            if p[i] == positive:\n",
    "                pred_corr += 1 \n",
    "                if p[i] == t[i]:\n",
    "                    corr += 1 \n",
    "        prec = corr / pred_corr if pred_corr > 0 else 0 \n",
    "        return round(prec, 5)\n",
    "\n",
    "    def binary_recall(self, positive=1):\n",
    "        '''\n",
    "        二类召回率的计算 \n",
    "        '''\n",
    "        p = self.pred_y\n",
    "        t = self.true_y\n",
    "        if isinstance(p[0], list):\n",
    "            p = [item[0] for item in p]\n",
    "        corr = 0 \n",
    "        true_corr = 0\n",
    "        for i in range(len(p)):\n",
    "            if t[i] == positive:\n",
    "                true_corr += 1 \n",
    "                if p[i] == t[i]:\n",
    "                    corr += 1 \n",
    "        rec = corr / true_corr if true_corr > 0 else 0 \n",
    "        return round(rec, 5)\n",
    "\n",
    "    def binary_f_beta(self, beta=1.0, positive=1):\n",
    "        '''\n",
    "        二类的f_beta的计算\n",
    "        '''\n",
    "        precision = self.binary_precision(positive)\n",
    "        recall = self.binary_recall(positive)\n",
    "        try:\n",
    "            f_b = (1+ beta*beta) * precision * recall / (beta*beta*precision + recall)\n",
    "        except:\n",
    "            f_b = 0 \n",
    "        return round(f_b, 5)\n",
    "    \n",
    "    def multi_precision(self):\n",
    "        '''\n",
    "        多类精确率的计算\n",
    "        '''\n",
    "        precisions = [self.binary_precision(label) for label in self.labels]\n",
    "        prec = mean(precisions)\n",
    "        return round(prec, 5)\n",
    "    \n",
    "    def multi_recall(self):\n",
    "        '''\n",
    "        多类召回率的计算 \n",
    "        '''\n",
    "        recalls = [self.binary_recall(label) for label in self.labels]\n",
    "        rec = mean(recalls)\n",
    "        return round(rec, 5)\n",
    "    \n",
    "    def multi_f_beta(self, beta=1.0):\n",
    "        '''\n",
    "        多类f_beta的计算\n",
    "        '''\n",
    "        f_betas = [self.binary_f_beta(beta, label) for label in labels]\n",
    "        f_beta = mean(f_betas)\n",
    "        return round(f_beta, 5)\n",
    "    \n",
    "    def get_binary_metrics(self, f_beta=1.0):\n",
    "        '''\n",
    "        得到二类的性能指标 \n",
    "        '''\n",
    "        metrics = {\"accuracy\": self.accuracy(), \"recall\": self.binary_recall(),\n",
    "                  \"precision\": self.binary_precision(), \"f_beta\": self.binary_f_beta(f_beta)}\n",
    "        return metrics\n",
    "    \n",
    "    def get_multi_metrics(self, f_beta=1.0):\n",
    "        '''\n",
    "        得到多类的性能指标 \n",
    "        '''\n",
    "        metrics = {\"accuracy\": self.accuracy(), \"recall\": self.multi_recall(), \n",
    "                  \"precision\": self.multi_precision(), \"f_beta\": self.multi_f_beta(f_beta)}\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:30:23.977412Z",
     "start_time": "2019-08-01T14:30:23.963967Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextCNN(BaseModel):\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "        super(TextCNN, self).__init__(config)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        self.build_model()\n",
    "        self.init_saver()\n",
    "        \n",
    "    def build_model(self):\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, self.config['sequenceLength']], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutProb = tf.placeholder(tf.float32, name=\"dropoutProb\")\n",
    "        \n",
    "        \n",
    "        # 定义L2损失值\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            ## 利用预训练的词向量，设置trainable=True，表示不冻结可以训练\n",
    "            self.W = tf.Variable(tf.cast(self.wordEmbedding, dtype=tf.float32, name=\"word2Vec\"), \n",
    "                                 name=\"W\", trainable=False)\n",
    "            ## 利用词嵌入矩阵将输入的数据中的词转换成词向量，输出为 [batch, seq_len, embed_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            ## 卷积的输入形状是[batch, heigth, width, channel]，所以需要扩维\n",
    "            self.embeddedWordsExpand = tf.expand_dims(self.embeddedWords, -1)\n",
    "            \n",
    "        # 创建卷积层和池化层\n",
    "        pooledOutputs = []\n",
    "        ## 根据自己定义的不同的filter_size，将输出进行融合\n",
    "        for i, filter_size, in enumerate(self.config['filterSizes']):\n",
    "            ## 卷积层，卷积核尺寸为 filter_size * embeded_size\n",
    "            filter_shape = [filter_size, self.config['embeddingSize'], 1, self.config['numFilters']]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[self.config['numFilters']]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(self.embeddedWordsExpand, W, strides=[1,1,1,1],\n",
    "                               padding=\"VALID\", name=\"conv\")\n",
    "            ## 利用relu进行非线性映射\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            \n",
    "            ## 池化层，进行最大池化之后得到一个值\n",
    "            ## 其中ksize的形状表示 [batch, height, width, channels]\n",
    "            pooled = tf.nn.max_pool(h, ksize=[1, self.config['sequenceLength']-filter_size+1, 1, 1],\n",
    "                                   strides=[1,1,1,1], padding=\"VALID\", name=\"pool\")\n",
    "            pooledOutputs.append(pooled)\n",
    "            \n",
    "        # 得到CNN网络的输出长度\n",
    "        numFiltersTotal = self.config['numFilters'] * len(self.config['filterSizes'])\n",
    "        ## 池化之后维度为[batch, 1, 1, channels]，按照最后一维进行concat\n",
    "        self.hPool = tf.concat(pooledOutputs, 3)\n",
    "        ## 摊平成二维数据输入到全连接层\n",
    "        self.hPoolFlat = tf.reshape(self.hPool, [-1, numFiltersTotal])\n",
    "        \n",
    "        # Dropout层\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.hDrop = tf.nn.dropout(self.hPoolFlat, self.dropoutProb)\n",
    "        \n",
    "        # 全连接层\n",
    "        with tf.name_scope(\"output\"):\n",
    "\n",
    "            self.logits = tf.layers.dense(self.hDrop, self.config['numClasses'], name=\"dense\",\n",
    "                                         kernel_initializer=tf.truncated_normal_initializer(stddev=0.1, seed=2019),\n",
    "                                         bias_initializer=tf.constant_initializer(0.1))\n",
    "            ## 获取该层的权重\n",
    "            with tf.variable_scope(\"dense\", reuse=True):\n",
    "                outputW = tf.get_variable(\"kernel\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            \n",
    "            if self.config['numClasses'] == 1: \n",
    "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.int32, name=\"predictions\")\n",
    "            elif self.config['numClasses'] > 1: \n",
    "                self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
    "            \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if self.config['numClasses'] == 1: \n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                labels=tf.cast(tf.reshape(self.inputY, [-1, 1]),\n",
    "                                                                              dtype=tf.float32))\n",
    "            elif self.config['numClasses'] > 1: \n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                       labels=self.inputY)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(losses) + self.config[\"l2RegLambda\"] * l2Loss           \n",
    "            \n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.train_step = tf.train.AdamOptimizer(\n",
    "                    self.config['learningRate']).minimize(self.loss, global_step=self.global_step_tensor)\n",
    "            \n",
    "\n",
    "    def init_saver(self):\n",
    "        '''\n",
    "        初始化用于保存模型的对象\n",
    "        '''\n",
    "        self.saver = tf.train.Saver(max_to_keep=self.config['max_to_keep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:30:24.494269Z",
     "start_time": "2019-08-01T14:30:24.419690Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer(BaseTrain):\n",
    "    def __init__(self, sess, model, data, config, logger, labels):\n",
    "        '''\n",
    "        这里的data要求是元组的形式，data[0]表示train对象，data[1]表示eval对象\n",
    "        '''\n",
    "        super(Trainer, self).__init__(sess, model, data, config, logger)\n",
    "        self.train = data[0]\n",
    "        self.eval = data[1]\n",
    "        self.labels = labels\n",
    "        #print(\"初始化结束...\")\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        num_iter_per_epoch = self.train.length // self.config[\"batch_size\"]\n",
    "\n",
    "        for _ in tqdm(range(num_iter_per_epoch)):\n",
    "            loss, metrics, step = self.train_step()\n",
    "            train_acc = metrics['accuracy']\n",
    "            train_f_score = metrics['f_beta']\n",
    "            \n",
    "            ## 将训练过程中的损失写入\n",
    "            summaries_dict = {\"loss\": loss, \n",
    "                              \"acc\": np.array(train_acc), \n",
    "                              \"f_score\": np.array(train_f_score)}\n",
    "            self.logger.summarize(step, summarizer=\"train\", scope=\"train_summary\", summaries_dict=summaries_dict)\n",
    "            \n",
    "            if step % self.config['evaluateEvery'] == 0: \n",
    "                print(\"Train —— Step: {} | Loss: {} | Acc: {} | F1_Score: {}\".format(\n",
    "                        step, loss, train_acc, train_f_score))\n",
    "                ## 对测试集进行评估\n",
    "                print(\"\\nEvaluation: \\n\")\n",
    "                eval_losses = []\n",
    "                eval_precs = []\n",
    "                eval_recalls = []\n",
    "                for batchEval in self.eval.iter_all(self.config[\"batch_size\"]):\n",
    "                    loss, metrics = self.eval_step(batchEval[0], batchEval[1])\n",
    "                    eval_losses.append(loss)\n",
    "                    eval_precs.append(metrics['precision'])\n",
    "                    eval_recalls.append(metrics[\"recall\"])\n",
    "                loss_mean = np.round(np.mean(eval_losses), 5)\n",
    "                prec_mean = np.round(np.mean(eval_precs), 5)\n",
    "                recall_mean = np.round(np.mean(eval_recalls),5)\n",
    "                time_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S %p\")\n",
    "                \n",
    "                print(\"{} | Loss: {} | Precision: {} | Recall: {}\".format(time_str,\n",
    "                                                                     loss_mean,\n",
    "                                                                     prec_mean, recall_mean))\n",
    "                summaries_dict = {\"loss\": np.array(loss_mean), \n",
    "                                  \"precision\": np.array(prec_mean), \n",
    "                                  \"recall\": np.array(recall_mean)}\n",
    "                self.logger.summarize(step, summarizer=\"test\", scope=\"test_summary\", summaries_dict=summaries_dict)\n",
    "            \n",
    "            if step % self.config[\"checkpointEvery\"] == 0: \n",
    "                self.model.save(self.sess)\n",
    "        \n",
    "    \n",
    "    def train_step(self):\n",
    "        batch_x, batch_y = next(self.train.next_batch(self.config[\"batch_size\"]))\n",
    "        feed_dict = {self.model.inputX: batch_x, self.model.inputY: batch_y,\n",
    "                    self.model.dropoutProb: self.config['dropoutProb']}\n",
    "        \n",
    "        _, loss, predicitons, step = self.sess.run([self.model.train_step, self.model.loss, \n",
    "                                                   self.model.predictions, self.model.global_step_tensor],\n",
    "                                                  feed_dict=feed_dict)\n",
    "        \n",
    "        getMetric = Metric(predicitons, batch_y, labels=self.labels)\n",
    "        if self.config['numClasses'] == 1: \n",
    "            metrics = getMetric.get_binary_metrics()\n",
    "        elif self.config['numClasses'] > 1: \n",
    "            metrics = getMetric.get_multi_metrics()\n",
    "        \n",
    "        \n",
    "        return loss, metrics, step\n",
    "    \n",
    "    def eval_step(self, batch_x, batch_y):\n",
    "        '''\n",
    "        使用验证集进行测试\n",
    "        '''\n",
    "        feed_dict = {self.model.inputX: batch_x, self.model.inputY: batch_y,\n",
    "                     self.model.dropoutProb: 1.0}\n",
    "        loss, predictions = self.sess.run([self.model.loss, self.model.predictions],\n",
    "                                          feed_dict=feed_dict)\n",
    "        \n",
    "        getMetric = Metric(predictions, batch_y, labels=self.labels)\n",
    "        if self.config['numClasses'] == 1: \n",
    "            metrics = getMetric.get_binary_metrics()\n",
    "        elif self.config['numClasses'] > 1: \n",
    "            metrics = getMetric.get_multi_metrics()\n",
    "        \n",
    "        return loss, metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:30:25.140610Z",
     "start_time": "2019-08-01T14:30:25.135501Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dirs(dirs):\n",
    "    try:\n",
    "        for dir_ in dirs: \n",
    "            if not os.path.exists(dir_):\n",
    "                os.makedirs(dir_)\n",
    "        return 0 \n",
    "    except Exception as e: \n",
    "        print(\"Creating directories error: {}\".format(e))\n",
    "        exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:30:25.767103Z",
     "start_time": "2019-08-01T14:30:25.737854Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 实例化配置参数对象\n",
    "    ## 指定训练数据的文件名\n",
    "    path = \"../data/imdb/labeldTrain.csv\"\n",
    "    config = Config(path)\n",
    "    \n",
    "    \n",
    "    create_dirs([config[\"summary_dir\"], config[\"checkpoint_dir\"]])\n",
    "    \n",
    "    data = Dataset(config)\n",
    "    data.dataGen(\"../data/imdb\", prefix=\"imdb\")\n",
    "    \n",
    "    train_X, train_y, eval_X, eval_y = data.trainReviews, data.trainLabels, data.evalReviews, data.evalLabels\n",
    "    wordEmbedding, labels = data.wordEmbedding, data.labelList\n",
    "    \n",
    "    train_data = DataGenerator(train_X, train_y)\n",
    "    eval_data = DataGenerator(eval_X, eval_y)\n",
    "    pack_data = [train_data, eval_data]\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    ## 设置计算图的配置\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction=0.9  # 配置GPU占用率\n",
    "    \n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    ## 创建一个实例\n",
    "    model = TextCNN(config, wordEmbedding)\n",
    "    \n",
    "    logger = Logger(sess, config)\n",
    "    \n",
    "    trainer = Trainer(sess, model, pack_data, config, logger, labels)\n",
    "    \n",
    "    \n",
    "    trainer.train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:31:39.588508Z",
     "start_time": "2019-08-01T14:30:26.682402Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-9-3997500fc990>:57: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-9-3997500fc990>:64: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "\n",
      "当前正处于第1次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15661ecc03c741f2b5f51549b60c0430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 100 | Loss: 0.9355392456054688 | Acc: 0.65625 | F1_Score: 0.60714\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:30:35 PM | Loss: 0.5347999930381775 | Precision: 0.88887 | Recall: 0.50664\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 200 | Loss: 0.5109789371490479 | Acc: 0.71875 | F1_Score: 0.76316\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:30:39 PM | Loss: 0.3994700014591217 | Precision: 0.7847 | Recall: 0.89907\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 300 | Loss: 0.3632057309150696 | Acc: 0.82812 | F1_Score: 0.81356\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:30:43 PM | Loss: 0.36473000049591064 | Precision: 0.87964 | Recall: 0.79756\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第2次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb53c6c309c46a7a99c66186b83b540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 400 | Loss: 0.3466982841491699 | Acc: 0.85938 | F1_Score: 0.85714\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:30:46 PM | Loss: 0.3438900113105774 | Precision: 0.88986 | Recall: 0.81893\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 500 | Loss: 0.459483802318573 | Acc: 0.76562 | F1_Score: 0.73684\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:30:50 PM | Loss: 0.327129989862442 | Precision: 0.88673 | Recall: 0.84108\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 600 | Loss: 0.27016669511795044 | Acc: 0.84375 | F1_Score: 0.83871\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:30:53 PM | Loss: 0.31435999274253845 | Precision: 0.87966 | Recall: 0.86597\n",
      "Saving model...\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第3次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb5b7723d32439c9762da46e44bcc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 700 | Loss: 0.39052173495292664 | Acc: 0.84375 | F1_Score: 0.85294\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:30:57 PM | Loss: 0.31248000264167786 | Precision: 0.84204 | Recall: 0.9132\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 800 | Loss: 0.2825438380241394 | Acc: 0.90625 | F1_Score: 0.90909\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:31:00 PM | Loss: 0.30691999197006226 | Precision: 0.88999 | Recall: 0.85598\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 900 | Loss: 0.2377379834651947 | Acc: 0.90625 | F1_Score: 0.875\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:31:04 PM | Loss: 0.31589001417160034 | Precision: 0.90855 | Recall: 0.82587\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第4次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e628ad42cdcb4161a7bf0b08a9f21d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 1000 | Loss: 0.17894205451011658 | Acc: 0.98438 | F1_Score: 0.98666\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:31:07 PM | Loss: 0.2924500107765198 | Precision: 0.864 | Recall: 0.90074\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1100 | Loss: 0.16951054334640503 | Acc: 0.95312 | F1_Score: 0.95774\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:31:10 PM | Loss: 0.2902800142765045 | Precision: 0.88512 | Recall: 0.8739\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1200 | Loss: 0.1722017526626587 | Acc: 0.9375 | F1_Score: 0.94118\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:31:14 PM | Loss: 0.29203999042510986 | Precision: 0.85152 | Recall: 0.92189\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第5次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0672d937d60a417db0ed0d9b4f436d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 1300 | Loss: 0.29393380880355835 | Acc: 0.85938 | F1_Score: 0.85246\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:31:17 PM | Loss: 0.28633999824523926 | Precision: 0.86488 | Recall: 0.9103\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1400 | Loss: 0.172616109251976 | Acc: 0.9375 | F1_Score: 0.94118\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:31:21 PM | Loss: 0.2852500081062317 | Precision: 0.89341 | Recall: 0.87313\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1500 | Loss: 0.1454441249370575 | Acc: 0.95312 | F1_Score: 0.95082\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:31:25 PM | Loss: 0.2804200053215027 | Precision: 0.87405 | Recall: 0.89898\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第6次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbd2031e68c4b9888d01ce2b2355612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 1600 | Loss: 0.10753684490919113 | Acc: 0.96875 | F1_Score: 0.96875\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:31:29 PM | Loss: 0.28185999393463135 | Precision: 0.86388 | Recall: 0.90984\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1700 | Loss: 0.13584646582603455 | Acc: 0.96875 | F1_Score: 0.97059\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:31:33 PM | Loss: 0.2865000069141388 | Precision: 0.89903 | Recall: 0.8614\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1800 | Loss: 0.20678932964801788 | Acc: 0.89062 | F1_Score: 0.88524\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-01 22:31:37 PM | Loss: 0.30316999554634094 | Precision: 0.83768 | Recall: 0.93436\n",
      "Saving model...\n",
      "Model saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不冻结word-embedding的情况下，验证集最优结果为 —— P: 0.89297, R: 0.88057 \n",
    "\n",
    "- 冻结word-embedding的情况下，验证集最优结果为  —— P: 0.89903, R: 0.8614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:46:34.451053Z",
     "start_time": "2019-08-01T14:46:34.441472Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(x, config):\n",
    "    with open(\"../data/wordJson/imdbword2idx.json\", \"r\", encoding=\"utf-8\") as f: \n",
    "        word2idx = json.load(f)\n",
    "    with open(\"../data/wordJson/imdblabel2idx.json\", \"r\", encoding=\"utf-8\") as f: \n",
    "        label2idx = json.load(f)\n",
    "        \n",
    "    idx2label  = {value:key for key, value in label2idx.items()}\n",
    "    \n",
    "    xIds = [word2idx.get(item, word2idx[\"UNK\"]) for item in x.split(\" \")]\n",
    "    if len(xIds) >= config[\"sequenceLength\"]:\n",
    "        xIds = xIds[:config[\"sequenceLength\"]]\n",
    "    else:\n",
    "        xIds = xIds + [word2idx[\"PAD\"]] * (config[\"sequenceLength\"] - len(xIds))\n",
    "    \n",
    "    g = tf.Graph()\n",
    "    \n",
    "    with g.as_default():\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "        session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False, \n",
    "                                     gpu_options=gpu_options)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            checkpoint_file = tf.train.latest_checkpoint(\"../model/textCNN/checkpoint/\")\n",
    "            saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "            saver.restore(sess, checkpoint_file)\n",
    "            \n",
    "            # 获取需要未给模型的参数\n",
    "            inputX = g.get_operation_by_name(\"inputX\").outputs[0]\n",
    "            dropoutProb = g.get_operation_by_name(\"dropoutProb\").outputs[0]\n",
    "            \n",
    "            # 获取输出的结果\n",
    "            predictions = g.get_tensor_by_name(\"output/predictions:0\")\n",
    "            pred = sess.run(predictions, feed_dict={inputX: [xIds], dropoutProb: 1.0})[0]\n",
    "            \n",
    "            print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T14:46:35.543409Z",
     "start_time": "2019-08-01T14:46:35.029441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/textCNN/checkpoint/my_model-1800\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "x = \"this is bad !\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "predict(x, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
