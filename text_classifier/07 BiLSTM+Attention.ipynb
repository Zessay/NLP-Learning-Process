{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T09:21:07.585233Z",
     "start_time": "2019-08-23T09:21:06.625805Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import time \n",
    "import datetime \n",
    "import logging\n",
    "import json \n",
    "import random\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from collections import Counter\n",
    "import gensim\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T09:21:07.665167Z",
     "start_time": "2019-08-23T09:21:07.589792Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义参数配置类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T07:41:35.299177Z",
     "start_time": "2019-08-09T07:41:35.294750Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, path=None):\n",
    "        super().__init__()\n",
    "        ## 定义训练参数\n",
    "        self['num_epochs'] = 5 \n",
    "        self['evaluateEvery'] = 100 \n",
    "        self['checkpointEvery'] = 100 \n",
    "        self['learningRate'] = 0.001 \n",
    "        \n",
    "        ## 定义模型参数\n",
    "        self['embeddingSize'] = 200 \n",
    "        \n",
    "        self['hiddenSizes'] = [256, 128]\n",
    "        self['dropoutProb'] = 0.5 \n",
    "        self['l2RegLambda'] = 0.0 \n",
    "        \n",
    "        ## 定义基础参数\n",
    "        self['sequenceLength'] = 200 \n",
    "        self['batch_size'] = 64 \n",
    "        self['dataSource'] = path\n",
    "        self['stopWordSource'] = \"../data/english\"\n",
    "        self['numClasses'] = 1  \n",
    "        self['train_size'] = 0.8   # 训练集和测试集比例\n",
    "        self.threshold = 0.5 \n",
    "        \n",
    "        ## 保存模型参数\n",
    "        self['checkpoint_dir'] = \"../model/BiLSTM_Attention/imdb/checkpoint\"\n",
    "        self['summary_dir'] = \"../model/BiLSTM_Attention/imdb/summary\"\n",
    "        self['max_to_keep'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T09:07:34.660014Z",
     "start_time": "2019-08-09T09:07:34.632703Z"
    }
   },
   "outputs": [],
   "source": [
    "class BiLSTMAttention(BaseModel):\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "        super(BiLSTMAttention, self).__init__(config)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        self.build_model()\n",
    "        self.init_saver()\n",
    "    \n",
    "    def build_model(self):\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, self.config[\"sequenceLength\"]], name=\"inpuX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutProb = tf.placeholder(tf.float32, name=\"dropoutProb\")\n",
    "        \n",
    "        # 定义L2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(self.wordEmbedding, dtype=tf.float32, name=\"word2vec\"),\n",
    "                                name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            for idx, hiddenSize in enumerate(self.config[\"hiddenSizes\"]):\n",
    "                with tf.name_scope(f\"Bi-LSTM{idx}\"):\n",
    "                    ## 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(\n",
    "                        num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                              output_keep_prob=self.dropoutProb)\n",
    "                    ## 定义后向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(\n",
    "                        num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                              output_keep_prob=self.dropoutProb)\n",
    "                    \n",
    "                    ## 采用动态rnn，可以动态输入序列长度\n",
    "                    ## 输出outputs的形式是[output_fw, output_bw]\n",
    "                    ## 其中两个元素的维度都是[batch_size, max_time, hidden_size]\n",
    "                    outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell,\n",
    "                                                                                 lstmBwCell,\n",
    "                                                                                 self.embeddedWords,\n",
    "                                                                                 dtype=tf.float32,\n",
    "                                                                                 scope=f\"bilstm-{idx}\")\n",
    "                    ## 对outputs的fw和bw的结果拼接 [batch_size, time_step, hidden_size*2]，\n",
    "                    ## 传入到下一层Bi-Lstm中\n",
    "                    self.embeddedWords = tf.concat(outputs, 2)\n",
    "        ## 将最后一层的Bi-LSTm结果分割成前向和后向\n",
    "        ## 第一个参数是分割的对象，第二个参数是分割之后的数量\n",
    "        ## 第三个参数是分割的维度\n",
    "        outputs = tf.split(self.embeddedWords, 2, -1)\n",
    "        \n",
    "        # 在Bi-LSTM + Attention论文中，将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            ## 这个张量的维度为[batch_size, max_time, hidden_size[-1]]\n",
    "            H = outputs[0] + outputs[1]\n",
    "            ## 得到Attention输出\n",
    "            output = self.attention(H)\n",
    "            ## 获取最后输出的维度\n",
    "            outputSize = self.config['hiddenSizes'][-1]\n",
    "            \n",
    "        ## 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            self.logits = tf.layers.dense(output, self.config['numClasses'],name=\"dense\",\n",
    "                                         kernel_initializer=tf.truncated_normal_initializer(stddev=0.1, seed=2019),\n",
    "                                         bias_initializer=tf.constant_initializer(0.1))\n",
    "            \n",
    "            ## 获取该层的权重\n",
    "            with tf.variable_scope(\"dense\", reuse=True):\n",
    "                outputW = tf.get_variable(\"kernel\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "        \n",
    "            if self.config['numClasses'] == 1: \n",
    "                self.predictions = tf.sigmod(self.logits)\n",
    "            elif self.config['numClasses'] > 1: \n",
    "                self.predictions = tf.nn.softmax(self.logits, dim=1)\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if self.config['numClasses'] == 1: \n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                labels=tf.cast(tf.reshape(self.inputY, [-1, 1]),\n",
    "                                                                              dtype=tf.float32))\n",
    "            elif self.config['numClasses'] > 1: \n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                       labels=self.inputY)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(losses) + self.config[\"l2RegLambda\"] * l2Loss\n",
    "            \n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.train_op = tf.train.AdamOptimizer(\n",
    "                    self.config['learningRate']).minimize(self.loss, global_step=self.global_step_tensor)\n",
    "            \n",
    "    ## 定义attention函数\n",
    "    def attention(self, H): \n",
    "        '''\n",
    "        利用Attention机制得到句子的向量表示\n",
    "        '''\n",
    "        hiddenSize = self.config[\"hiddenSizes\"][-1]\n",
    "        ## 初始化一个权重向量，用于和每个时间步的向量做attention\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        \n",
    "        # 对BiLSTM的输出用激活函数做非线性变化\n",
    "        M = tf.tanh(H)\n",
    "        # 定义W和M做矩阵运算，M的形状是[batch_size, max_time, hidden_size]\n",
    "        # W的形状是 [hidden_size]，希望得到输出[batch_size, max_time, 1]\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # 对newM做维度转换变为 [batch_size, max_time]\n",
    "        restoreM = tf.reshape(newM, [-1, self.config[\"sequenceLength\"]])\n",
    "        \n",
    "        # 用softmax归一化处理[batch_size, max_time]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的alpha的值对H进行加权求和，用矩阵运算直接操作\n",
    "        ## 转换后的H维度为[batch_size, hidden_size, max_time]\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.expand_dims(self.alpha, 2))\n",
    "        \n",
    "        ## 将三维压缩成二维 [batch_size, hidden_size]\n",
    "        sequeezeR = tf.reshape(r, [-1, hiddenSize])\n",
    "        sentenceRepren = tf.tanh(sequeezeR)\n",
    "        \n",
    "        # 对Attention做dropout处理\n",
    "        output = tf.nn.dropout(sentenceRepren, self.dropoutProb)\n",
    "        return output\n",
    "    \n",
    "    def init_saver(self):\n",
    "        '''\n",
    "        初始化用于保存模型的对象\n",
    "         '''\n",
    "        self.saver = tf.train.Saver(max_to_keep=self.config[\"max_to_keep\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T09:07:34.930400Z",
     "start_time": "2019-08-09T09:07:34.916492Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer(BaseTrain):\n",
    "    def __init__(self, sess, model, data, config, logger):\n",
    "        super(Trainer, self).__init__(sess, model, data, config, logger)\n",
    "        self.train = data[0]\n",
    "        self.eval = data[1]\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        num_iter_per_epoch = self.train.length // self.config[\"batch_size\"]\n",
    "        \n",
    "        for _ in tqdm(range(num_iter_per_epoch)):\n",
    "            ## 获取训练过程的结果\n",
    "            loss, metrics, step = self.train_step()\n",
    "            train_acc = metrics[\"accuracy\"]\n",
    "            train_f_score = metrics[\"f_score\"]\n",
    "            \n",
    "            ## 将训练过程中的损失写入\n",
    "            summaries_dict = {\"loss\": loss, \n",
    "                             \"acc\": np.array(train_acc), \n",
    "                             \"f_score\": np.array(train_f_score)}\n",
    "            self.logger.summarize(step, summarizer=\"train\", scope=\"train_summary\",\n",
    "                                 summaries_dict=summaries_dict)\n",
    "            \n",
    "            if step % self.config['evaluateEvery'] == 0:\n",
    "                print(\"Train —— Step: {} | Loss: {} | Acc: {} | F1_Score: {}\".format(\n",
    "                    step, loss, train_acc, train_f_score))\n",
    "                ## 对测试集进行评估\n",
    "                print(\"\\nEvaluation: \\n\")\n",
    "                eval_losses = []\n",
    "                eval_true = []\n",
    "                eval_pred = []\n",
    "                \n",
    "                for batchEval in self.eval.iter_all(self.config[\"batch_size\"]):\n",
    "                    loss, precdictions = self.eval_step(batchEval[0], batchEval[1])\n",
    "                    eval_losses.append(loss)\n",
    "                    eval_true.extend(batchEval[-1])\n",
    "                    eval_pred.extend(precdictions)\n",
    "                getMetric = Metric(np.array(eval_pred), np.array(eval_true), self.config)\n",
    "                metrics = getMetric.get_metrics()\n",
    "                \n",
    "                prec_mean = np.round(metrics['precision'])\n",
    "                recall_mean = np.round(metrics['recall'])\n",
    "                loss_mean = np.round(np.mean(eval_losses), 5)\n",
    "                time_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M%S %p\")\n",
    "                \n",
    "                print(\"{} | Loss: {} | Precision: {} | Recall: {}\".format(time_str,\n",
    "                                                                         loss_mean,\n",
    "                                                                         prec_mean,\n",
    "                                                                         recall_mean))\n",
    "                \n",
    "                summaries_dict = {\"loss\": np.array(loss_mean),\n",
    "                                 \"precision\": np.array(prec_mean),\n",
    "                                 \"recall\": np.array(recall_mean)}\n",
    "                self.logger.summarize(step, summarizer=\"test\", scope=\"test_summary\",\n",
    "                                     summaries_dict=summaries_dict)\n",
    "                \n",
    "            if step % self.config[\"checkpointEvery\"] == 0: \n",
    "                self.model.save(self.sess)\n",
    "    \n",
    "    \n",
    "    def train_step(self):\n",
    "        batch_x, batch_y = next(self.train.next_batch(self.config[\"batch_size\"]))\n",
    "        feed_dict = {self.model.inputX: batch_x, \n",
    "                    self.model.inputY: batch_y, \n",
    "                    self.model.dropoutProb: self.config[\"dropoutProb\"]}\n",
    "        \n",
    "        _, loss, predictions, step = self.sess.run([self.model.train_op, \n",
    "                                                   self.model.loss, \n",
    "                                                   self.model.predictions,\n",
    "                                                   self.model.global_step_tensor],\n",
    "                                                  feed_dict=feed_dict)\n",
    "        getMetric = Metric(predictions, batch_y, self.config)\n",
    "        metrics = getMetric.get_metrics()\n",
    "            \n",
    "        return loss, metrics, step \n",
    "    \n",
    "    def eval_step(self, batch_x, batch_y):\n",
    "        '''\n",
    "        使用验证集数据进行测试\n",
    "        '''\n",
    "        feed_dict = {self.model.inputX: batch_x, self.model.inputY: batch_y,\n",
    "                    self.model.dropoutProb: 1.0}\n",
    "        loss, predictions = self.sess.run([self.model.loss, self.model.predictions],\n",
    "                                         feed_dict=feed_dict)\n",
    "            \n",
    "        return loss, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用数据集进行训练和预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用IMDB数据集进行训练和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T09:08:08.434271Z",
     "start_time": "2019-08-09T09:08:08.427056Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 实例化配置参数对象\n",
    "    ## 指定训练数据的文件名\n",
    "    path = \"../data/imdb/labeldTrain.csv\"\n",
    "    config = Config(path)\n",
    "    \n",
    "    create_dirs([config[\"summary_dir\"], config[\"checkpoint_dir\"]])\n",
    "    data = Dataset(config)\n",
    "    \n",
    "    ## 生成训练集数据，第一个参数表示wordembedding文件所在文件夹\n",
    "    data.dataGen(\"../data/imdb\", prefix=\"imdb\")\n",
    "    \n",
    "    train_X, train_y, eval_X, eval_y = data.trainReviews, data.trainLabels, data.evalReviews, data.evalLabels\n",
    "    wordEmbedding, labels = data.wordEmbedding, data.labelList\n",
    "    \n",
    "    train_data = DataGenerator(train_X, train_y)\n",
    "    eval_data = DataGenerator(eval_X, eval_y)\n",
    "    pack_data = [train_data, eval_data]\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    ## 设置计算图的配置\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9 \n",
    "    \n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    ## 创建一个实例\n",
    "    model = BiLSTMAttention(config, wordEmbedding)\n",
    "    \n",
    "    logger = Logger(sess, config)\n",
    "    \n",
    "    trainer = Trainer(sess, model, pack_data, config, logger)\n",
    "    \n",
    "    trainer.train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T09:18:34.647184Z",
     "start_time": "2019-08-09T09:08:08.707485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前正处于第1次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7260d36fab7b441cbb84b3f142428a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 100 | Loss: 0.4060550034046173 | Acc: 0.85938 | F1_Score: 0.87671\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:0858 PM | Loss: 0.3330399990081787 | Precision: 0.83999 | Recall: 0.89307\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 200 | Loss: 0.36316460371017456 | Acc: 0.82812 | F1_Score: 0.8254\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:0944 PM | Loss: 0.3258199989795685 | Precision: 0.81149 | Recall: 0.94902\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 300 | Loss: 0.3440439701080322 | Acc: 0.82812 | F1_Score: 0.84507\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:1026 PM | Loss: 0.3162600100040436 | Precision: 0.82799 | Recall: 0.94431\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第2次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fdc2926d1048e68a2808a5ac67a55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 400 | Loss: 0.20181700587272644 | Acc: 0.95312 | F1_Score: 0.94737\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:1105 PM | Loss: 0.2894200086593628 | Precision: 0.91463 | Recall: 0.83423\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 500 | Loss: 0.1569727063179016 | Acc: 0.95312 | F1_Score: 0.94737\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:1145 PM | Loss: 0.36614999175071716 | Precision: 0.84197 | Recall: 0.93512\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 600 | Loss: 0.29701024293899536 | Acc: 0.875 | F1_Score: 0.85185\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:1223 PM | Loss: 0.3371700048446655 | Precision: 0.91406 | Recall: 0.8234\n",
      "Saving model...\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第3次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3c6ef3cc2c42759d4bef62ed23c542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 700 | Loss: 0.15801744163036346 | Acc: 0.95312 | F1_Score: 0.94545\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:1302 PM | Loss: 0.3246600031852722 | Precision: 0.85958 | Recall: 0.91539\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 800 | Loss: 0.1503821313381195 | Acc: 0.96875 | F1_Score: 0.97059\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:1341 PM | Loss: 0.3363800048828125 | Precision: 0.88679 | Recall: 0.88327\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 900 | Loss: 0.14376845955848694 | Acc: 0.95312 | F1_Score: 0.95652\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:1419 PM | Loss: 0.4718399941921234 | Precision: 0.93944 | Recall: 0.74085\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第4次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3e29f4ab1745dfa69e1572cca5e974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 1000 | Loss: 0.04328688234090805 | Acc: 0.98438 | F1_Score: 0.97872\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:1458 PM | Loss: 0.4139600098133087 | Precision: 0.90191 | Recall: 0.84872\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1100 | Loss: 0.05956492945551872 | Acc: 0.96875 | F1_Score: 0.96429\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:1537 PM | Loss: 0.3738499879837036 | Precision: 0.89881 | Recall: 0.85989\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1200 | Loss: 0.05035770684480667 | Acc: 0.98438 | F1_Score: 0.98734\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:1617 PM | Loss: 0.3781299889087677 | Precision: 0.88545 | Recall: 0.87843\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第5次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6d489ec5414f199e09e24b17714275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 1300 | Loss: 0.02755913697183132 | Acc: 1.0 | F1_Score: 1.0\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:1657 PM | Loss: 0.4448400139808655 | Precision: 0.89958 | Recall: 0.84814\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1400 | Loss: 0.06449589133262634 | Acc: 0.96875 | F1_Score: 0.95833\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:1737 PM | Loss: 0.44767001271247864 | Precision: 0.86049 | Recall: 0.91544\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1500 | Loss: 0.03325489163398743 | Acc: 0.98438 | F1_Score: 0.98508\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:1816 PM | Loss: 0.4826500117778778 | Precision: 0.87725 | Recall: 0.88369\n",
      "Saving model...\n",
      "Model saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 最终结果 —— Precision: 0.87725, Recall: 0.88369"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Yelps数据集训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T09:37:56.008224Z",
     "start_time": "2019-08-09T09:37:56.001756Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    path = \"../data/yelps/yelps_test.csv\"\n",
    "    config = Config(path)\n",
    "    config[\"summary_dir\"] = \"../model/BiLSTM_Attention/yelps/summary\"\n",
    "    config[\"checkpoint_dir\"] = \"../model/BiLSTM_Attention/yelps/checkpoint\"\n",
    "    config['evaluateEvery'] = 2000 \n",
    "    config['checkpointEvery'] = 2000 \n",
    "    \n",
    "    \n",
    "    create_dirs([config[\"summary_dir\"], config[\"checkpoint_dir\"]])\n",
    "    \n",
    "    data = Dataset(config)\n",
    "    \n",
    "    ## 生成训练集数据\n",
    "    data.dataGen(\"../data/yelps/\", prefix=\"yelps\")\n",
    "    \n",
    "    train_X, train_y, eval_X, eval_y = data.trainReviews, data.trainLabels, data.evalReviews, data.evalLabels\n",
    "    wordEmbedding, labels = data.wordEmbedding, data.labelList\n",
    "    \n",
    "    train_data = DataGenerator(train_X, train_y)\n",
    "    eval_data = DataGenerator(eval_X, eval_y)\n",
    "    pack_data = [train_data, eval_data]\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # 设置计算图的配置\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, \n",
    "                                 log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9 \n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    ## 创建一个实例\n",
    "    model = BiLSTMAttention(config, wordEmbedding)\n",
    "    \n",
    "    logger = Logger(sess, config)\n",
    "    trainer = Trainer(sess, model, pack_data, config, logger)\n",
    "    \n",
    "    trainer.train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T11:48:46.078217Z",
     "start_time": "2019-08-09T09:37:56.204385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前正处于第1次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0fd754ae1345579695adfe8c7f4390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8939), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 2000 | Loss: 0.3246549963951111 | Acc: 0.84375 | F1_Score: 0.89362\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 17:5334 PM | Loss: 0.22502000629901886 | Precision: 0.91156 | Recall: 0.95389\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 4000 | Loss: 0.1653369516134262 | Acc: 0.95312 | F1_Score: 0.96703\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 18:0850 PM | Loss: 0.22026999294757843 | Precision: 0.92169 | Recall: 0.94862\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 6000 | Loss: 0.129671111702919 | Acc: 0.95312 | F1_Score: 0.96386\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 18:2351 PM | Loss: 0.21250000596046448 | Precision: 0.92785 | Recall: 0.94586\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 8000 | Loss: 0.19084835052490234 | Acc: 0.90625 | F1_Score: 0.925\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 18:3850 PM | Loss: 0.21105000376701355 | Precision: 0.92301 | Recall: 0.95253\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "当前正处于第2次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1e88fbbd73453b9dc57a12116e82f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8939), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 10000 | Loss: 0.15338148176670074 | Acc: 0.92188 | F1_Score: 0.93976\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 18:5353 PM | Loss: 0.2106499969959259 | Precision: 0.93676 | Recall: 0.93668\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 12000 | Loss: 0.10625214874744415 | Acc: 0.96875 | F1_Score: 0.97727\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 19:0903 PM | Loss: 0.2207300066947937 | Precision: 0.93263 | Recall: 0.93988\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 14000 | Loss: 0.22055131196975708 | Acc: 0.92188 | F1_Score: 0.93827\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 19:2407 PM | Loss: 0.2228900045156479 | Precision: 0.92721 | Recall: 0.9465\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 16000 | Loss: 0.13444609940052032 | Acc: 0.90625 | F1_Score: 0.92857\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-09 19:3847 PM | Loss: 0.2180500030517578 | Precision: 0.91951 | Recall: 0.95648\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "当前正处于第3次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a277c7f32a6f4cee92ffd98ea5e884be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8939), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-417f39cb5f52>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpack_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/python/nlp/text_classifier/utils/base_trainer.py\u001b[0m in \u001b[0;36mtrain_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcur_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_epoch_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n当前正处于第{cur_epoch + 1}次迭代\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;31m## 将对应的epoch+1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_cur_epoch_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-0d03ccb035b6>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iter_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m## 获取训练过程的结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mtrain_f_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f_beta\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-0d03ccb035b6>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                                    self.model.global_step_tensor],\n\u001b[0;32m---> 69\u001b[0;31m                                                   feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mgetMetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"numClasses\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 最佳结果 —— Precision：0.93676  Recall: 0.93668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
