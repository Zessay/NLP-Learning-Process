{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0905 10:55:22.463685 139948305139456 deprecation_wrapper.py:119] From /home/chen/anaconda3/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import tokenization\n",
    "from bert import optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 使用IMDB数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/imdb/labeldTrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练即和测试集\n",
    "#train_X, eval_X, train_y, eval_y = train_test_split(data['review'].values, data['sentiment'].values, shuffle=True, stratify=data['sentiment'].values)\n",
    "train = data[:int(data.shape[0]*0.8)]\n",
    "test = data[int(data.shape[0]*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;需要将数据转换成BERT可以理解的形式，主要分为两步。\n",
    "\n",
    "第一步，构造使用BERT的构造器构造`InputExample`的实例\n",
    "\n",
    "- `text_a`：表示要分类的文本\n",
    "- `text_b`：在计算两个语句关系的时候使用，比如翻译，问答等；所以这里只需要设置`text_b=None`\n",
    "- `label`：样本的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = \"review\"\n",
    "LABEL_COLUMN = \"sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里｀guid｀表示用于记录的全局唯一ID，本例中没有作用\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
    "                                                                            text_a=x[DATA_COLUMN],\n",
    "                                                                            text_b=None, \n",
    "                                                                            label=x[LABEL_COLUMN]),\n",
    "                                 axis=1)\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
    "                                                                          text_a=x[DATA_COLUMN],\n",
    "                                                                          text_b=None,\n",
    "                                                                          label=x[LABEL_COLUMN]),\n",
    "                               axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二部，对数据进行预处理，可以用于BERT的训练。\n",
    "\n",
    "- 将所有单词小写\n",
    "- 分词\n",
    "- 将words分割成**wordpieces**\n",
    "- 将单词转换为词表中的索引\n",
    "- 增加`CLS`和`SEP`字符\n",
    "- 对输入增加`pos-embedding`和`segment-embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0905 11:00:34.705354 139948305139456 deprecation_wrapper.py:119] From /home/chen/anaconda3/lib/python3.7/site-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                 tokenization_info[\"do_lower_case\"]])\n",
    "    \n",
    "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用上面定义好的分词器，需要用`run_classifier.convert_examples_to_features`将数据转换成BERT理解的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = data['sentiment'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0905 11:05:25.524482 139948305139456 deprecation_wrapper.py:119] From /home/chen/anaconda3/lib/python3.7/site-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义语句的最大长度\n",
    "MAX_SEQ_LENGTH = 128 \n",
    "# 将训练集和测试集转换成BERT理解的特征\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list,\n",
    "                                                                 MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list,\n",
    "                                                                MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 构造模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，再次加载`BERT hub`模型，这次是用来提取计算图；接着，创建一个新的层，训练BERT来用于情感分类任务。这种使用基本上已经训练好的模型的策略叫做`fine-tuning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels, num_labels):\n",
    "    '''构造一个分类器模型'''\n",
    "    bert_module = hub.Module(\n",
    "        BERT_MODEL_HUB,\n",
    "        trainable=True)\n",
    "    bert_inputs = dict(input_ids=input_ids,\n",
    "                      input_mask=input_mask,\n",
    "                      segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "        inputs=bert_inputs,\n",
    "        signature=\"tokens\",\n",
    "        as_dict=True)\n",
    "    \n",
    "    # 使用`pooled_output`用于句级别的分类任务\n",
    "    # 使用｀sequence_outputs｀用于词级别的任务\n",
    "    output_layer = bert_outputs['pooled_output']\n",
    "    ## 获取输出层的隐层大小\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "    \n",
    "    ## 构造我们自己的最终分类层\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.initializers.glorot_normal())\n",
    "    output_bias = tf.get_variable(\"output_bias\",\n",
    "                                 [num_labels],\n",
    "                                 initializer=tf.initializers.glorot_normal())\n",
    "    \n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        # 使用dropout\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        \n",
    "        # 将标签转换为one-hot的形式\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        \n",
    "        ## 如果是预测结果，输出预测标签以及对应的概率\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "        \n",
    "        ## 如果是训练或者评估节点，则计算损失和真实标签\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels*log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对模型进行包装，适用于训练、评估和预测\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps, num_warmup_steps):\n",
    "    def model_fn(features, labels, mode, params):\n",
    "        input_ids = features['input_ids']\n",
    "        input_mask = features['input_mask']\n",
    "        segment_ids = features['segment_ids']\n",
    "        label_ids = features['label_ids']\n",
    "        \n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "        # 如果是训练或者评估截断\n",
    "        if not is_predicting:\n",
    "            (loss, predicted_labels, log_probs) = create_model(is_predicting, input_ids,\n",
    "                                                              input_mask, segment_ids, label_ids, num_labels)\n",
    "            train_op = bert.optimization.create_optimizer(loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "            \n",
    "            ## 计算评估的metrics\n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                f1_score = tf.metrics.f1_score(label_ids, predicted_labels)\n",
    "                auc = tf.metrics.auc(label_ids, predicted_labels)\n",
    "                recall = tf.metrics.recall(label_ids, predicted_labels)\n",
    "                precision = tf.metrics.precision(label_ids, predicted_labels)\n",
    "                \n",
    "                true_pos = tf.metrics.true_positives(label_ids, predicted_labels)\n",
    "                true_neg = tf.metrics.true_negatives(label_ids, predicted_labels)\n",
    "                false_pos = tf.metrics.false_positives(label_ids, predicted_labels)\n",
    "                false_neg = tf.metrics.false_negatives(label_ids, predicted_labels)\n",
    "                \n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"f1_score\": f1_score,\n",
    "                    \"auc\": auc,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"true_positives\": true_pos,\n",
    "                    \"true_negatives\": true_neg,\n",
    "                    \"false_positives\": false_pos,\n",
    "                    \"fasle_negatives\": false_neg\n",
    "                }\n",
    "            \n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "            \n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics)\n",
    "            \n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(is_predicting, input_ids,\n",
    "                                                        input_mask, segment_ids, label_ids, num_labels)\n",
    "            predictions = {\n",
    "                \"probabilities\": log_probs,\n",
    "                \"labels\": predicted_labels\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "        \n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "BATCH_SIZE = 32 \n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0 \n",
    "WARMUP_PROPORTION = 0.1 \n",
    "SAVE_CHECKPOINTS_STEPS = 500 \n",
    "SAVE_SUMMARY_STEPS = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = int(len(train_features)/ BATCH_SIZE*NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps*WARMUP_PROPORTION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"../model/BERT/imdb/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dirs(paths):\n",
    "    for path in paths:\n",
    "        if os.path.exists(path):\n",
    "            os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dirs([OUTPUT_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置基础的配置\n",
    "run_config = tf.estimator.RunConfig(model_dir=OUTPUT_DIR, \n",
    "                                   save_summary_steps=SAVE_SUMMARY_STEPS, \n",
    "                                   save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "    num_labels=len(label_list),\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                  config=run_config,\n",
    "                                  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop_remainder=True表示使用TPU\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(features=train_features,\n",
    "                                                     seq_length=MAX_SEQ_LENGTH,\n",
    "                                                     is_training=True,\n",
    "                                                     drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 训练阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begining Training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0905 15:34:35.813420 139948305139456 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 48 vs previous value: 48. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "W0905 15:58:49.302065 139948305139456 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 63 vs previous value: 63. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "W0905 16:11:29.363326 139948305139456 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 71 vs previous value: 71. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "W0905 16:21:11.634079 139948305139456 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 77 vs previous value: 77. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "W0905 16:27:32.699530 139948305139456 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 81 vs previous value: 81. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Begining Training!\")\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time: \", datetime.now()-current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 评估阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = run_classifier.input_fn_builder(features=test_features,\n",
    "                                               seq_length=MAX_SEQ_LENGTH,\n",
    "                                               is_training=False,\n",
    "                                               drop_remainder=False)\n",
    "\n",
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 预测阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(in_sentences):\n",
    "    labels = [\"Negative\", \"Positive\"]\n",
    "    input_examples = [run_classifier.InputExample(guid=\"\", text_a=x, text_b=None, label=0) for x in in_sentences]\n",
    "    input_features = run_classifer.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    predict_input_fn = run_classfier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "    return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "    \"That movie was absolutely awful\",\n",
    "    \"The acting was a bit lacking\",\n",
    "    \"This film was creative and surprising\",\n",
    "    \"Absolutely fantastic!\"\n",
    "]\n",
    "\n",
    "predictions = getPrediction(pred_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
