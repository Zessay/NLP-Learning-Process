{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T08:06:19.980124Z",
     "start_time": "2019-08-22T08:06:19.783965Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import json \n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T08:06:21.797672Z",
     "start_time": "2019-08-22T08:06:20.966849Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import *  \n",
    "from collections import Counter\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T08:06:22.558442Z",
     "start_time": "2019-08-22T08:06:22.556634Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对数据集进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T08:06:24.370183Z",
     "start_time": "2019-08-22T08:06:24.204322Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import tokenize\n",
    "import os \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:38:23.515053Z",
     "start_time": "2019-08-22T06:38:23.505340Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 添加需要去除的标点符号集，问号和感叹号除外\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '?', '!', '|', ';', \"'\", '$', '&', '/', \n",
    "          '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', '·', '_', \n",
    "          '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', \n",
    "          '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', \n",
    "          '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥',\n",
    "          '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', \n",
    "          '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    "          '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', \n",
    "          '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "# 定义一些常见的缩写\n",
    "## 这里\"\\g<1>\"表示前面匹配模式group(1)的值，group(0)表示全部，group(1)表示第一个括号的匹配值\n",
    "contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'),\n",
    "                        (r'isn\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), \n",
    "                        (r'(\\w+)n\\'t', '\\g<1> not'),(r'(\\w+)\\'ve', '\\g<1> have'), \n",
    "                        (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), \n",
    "                        (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'),\n",
    "                        (r'dont', 'do not'), (r'wont', 'will not') ]\n",
    "\n",
    "def clean_text(text):\n",
    "    stoplists = stopwords.words(\"english\")\n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    # 去除对情感分类没有用的数字\n",
    "    text = re.sub(\"[0-9]+\", \"\", text)\n",
    "    # 对重复出现的标点进行天魂\n",
    "    text = re.sub(r\"(\\!)\\1+\", \"multiExclamation\", text)\n",
    "    text = re.sub(r\"(\\?)\\1+\", \"multiQuestion\", text)\n",
    "    text = re.sub(r\"(\\.)\\1+\", \"multiStop\", text)\n",
    "    \n",
    "    # 对缩写进行替换\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    \n",
    "    # 在标点前面加上空格\n",
    "    for punct in puncts:\n",
    "        text = text.replace(punct, f\" {punct} \")\n",
    "    \n",
    "    #print(text)\n",
    "    # 对文本进行分词\n",
    "    text_split = tokenize.word_tokenize(text)\n",
    "    text = [word for word in text_split if word not in stoplists]\n",
    "    text = [wnl.lemmatize(word) for word in text]\n",
    "    \n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:38:23.688940Z",
     "start_time": "2019-08-22T06:38:23.685332Z"
    }
   },
   "outputs": [],
   "source": [
    "## 获取用于训练的数据集\n",
    "def preprocess(path=\"../data/yelps/review.csv\"):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data[[\"stars\", \"text\"]]\n",
    "    data[\"emotion\"] = data[\"stars\"].apply(lambda x: int(x>3.0))\n",
    "    data = data[[\"text\", \"emotion\"]]\n",
    "    data.rename(columns={\"text\": \"review\"}, inplace=True)\n",
    "    # 取前10w的数据作为训练集\n",
    "    data = data[:100000]\n",
    "    \n",
    "    print(\"正在处理文本...\")\n",
    "    data[\"review\"] = data[\"review\"].apply(clean_text)\n",
    "    \n",
    "    print(\"文本处理结束!\")\n",
    "    data.to_csv(os.path.join(os.path.dirname(path), \"yelps.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T08:17:20.536589Z",
     "start_time": "2019-08-16T08:14:59.732062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文本...\n",
      "文本处理结束!\n"
     ]
    }
   ],
   "source": [
    "preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T09:18:07.417044Z",
     "start_time": "2019-08-21T09:18:07.122008Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/yelps/yelps.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T09:18:14.452460Z",
     "start_time": "2019-08-21T09:18:14.430284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total bill horrible service ? Over $ Gs . Thes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I * adore * Travis Hard Rock new Kelly Cardena...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I say office really together , organized frien...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Went lunch . Steak sandwich delicious , Caesar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Today second three session I paid . Although f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  emotion\n",
       "0  Total bill horrible service ? Over $ Gs . Thes...        0\n",
       "1  I * adore * Travis Hard Rock new Kelly Cardena...        1\n",
       "2  I say office really together , organized frien...        1\n",
       "3  Went lunch . Steak sandwich delicious , Caesar...        1\n",
       "4  Today second three session I paid . Although f...        0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义文本数据处理类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T08:06:29.457284Z",
     "start_time": "2019-08-22T08:06:29.453173Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T08:06:30.004827Z",
     "start_time": "2019-08-22T08:06:29.999360Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T08:08:02.043806Z",
     "start_time": "2019-08-22T08:08:01.952127Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config \n",
    "        self._dataSource = config[\"dataSource\"]  # 数据源\n",
    "        self._stopWordSource = config[\"stopWordSource\"]  # 停止词目录\n",
    "        self._sentenceLength = config[\"sentenceLength\"]   # 表示语句的长度，每个语句多少个单词\n",
    "        self._docLength = config[\"docLength\"]    # 表示文章的长度，每篇文章多少个句子\n",
    "        \n",
    "        self._embeddingSize = config[\"embeddingSize\"]\n",
    "        self._batchSize = config[\"batch_size\"]\n",
    "        self._trainRate = config[\"train_size\"]\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        self.trainReviews = []   # 存储训练集\n",
    "        self.trainLabels = []    # 存储训练集标签\n",
    "        self.evalReviews = []    # 存储验证集\n",
    "        self.evalLabels = []     # 存储验证集标签\n",
    "        \n",
    "        self.wordEmbedding = None  # 保存embedding的对照表\n",
    "        self.labelList = []      # 保存有多少不同的标签值\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        '''\n",
    "        从csv文件中读取数据集\n",
    "        '''\n",
    "        df = pd.read_csv(filePath)\n",
    "        if self.config['numClasses'] == 1:\n",
    "            if \"sentiment\" in df.columns:\n",
    "                labels = df[\"sentiment\"].tolist()\n",
    "            if \"emotion\" in df.columns:\n",
    "                labels = df[\"emotion\"].tolist()\n",
    "        elif self.config['numClasses'] > 1:\n",
    "            labels = df[\"rate\"].tolist()\n",
    "        \n",
    "        ## 获取所有的评论文本\n",
    "        data_x = []\n",
    "        reviews = df[\"review\"].tolist()\n",
    "        ## 对于每一个评论，都按照文章处理\n",
    "        for text in reviews:\n",
    "            doc = []\n",
    "            ## 按照语句进行划分得到不同的语句\n",
    "            sents = sent_tokenize(text)\n",
    "            for i, sent in enumerate(sents):\n",
    "                ## 对每个句子进行分词，并保存\n",
    "                doc.append(word_tokenize(sent))\n",
    "            ## 将每篇文章的分词结果保存\n",
    "            data_x.append(doc)\n",
    "            \n",
    "        return data_x, labels\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        '''\n",
    "        读取停用词\n",
    "        '''\n",
    "        with open(stopWordPath, \"r\") as f: \n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            ## 转换成字典的形式，使用hash查找效率更高\n",
    "            self._stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "    \n",
    "        \n",
    "    def _getWordEmbedding(self, words, path):\n",
    "        '''\n",
    "        按照数据集中的单词取出训练好的词向量 \n",
    "        '''\n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(path, \"wordvector.bin\"),\n",
    "                                                                 binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加\"pad\"和\"unk\"\n",
    "        vocab.append(\"PAD\")\n",
    "        vocab.append(\"UNK\")\n",
    "        \n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize)) # 用全0向量表示\"PAD\"\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))  # 用随机向量表示\"UNK\"\n",
    "        \n",
    "        for word in words:\n",
    "            if word != \"PAD\" and word != \"UNK\":\n",
    "                try:\n",
    "                    vector = wordVec.wv[word]\n",
    "                    vocab.append(word)\n",
    "                    wordEmbedding.append(vector)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _genVocabulary(self, reviews, labels, path, prefix=\"\"):\n",
    "        '''\n",
    "        生成向量以及词汇-索引的字典\n",
    "        '''\n",
    "        save_path = \"../data/wordJson/\"\n",
    "        target_word_dir = os.path.join(save_path, prefix+\"_word2idx.json\")\n",
    "        target_label_dir = os.path.join(save_path, prefix+\"_label2idx.json\")\n",
    "        target_freq_dir = os.path.join(save_path, prefix+\"_wordfreq.json\")\n",
    "        \n",
    "        ## 对于每一篇文章中，每一句话的每一个词\n",
    "        allWords = [word for doc in reviews for sent in doc for word in sent]\n",
    "        ## 去除停用词\n",
    "        subWords = [word for word in allWords if word not in self._stopWordDict]\n",
    "        ## 统计词频\n",
    "        wordCount = Counter(subWords)\n",
    "        ## 按照词频进行排序，去除低频词\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "\n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words, path)\n",
    "        self.wordEmbedding = wordEmbedding            \n",
    "        word2idx = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "        uniqueLabel = list(set(labels))\n",
    "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
    "        self.labelList = list(range(len(uniqueLabel)))\n",
    "\n",
    "        ## 将词汇表-索引表保存为json数据\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        with open(target_word_dir, \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(word2idx, f)\n",
    "\n",
    "        with open(target_label_dir, \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(label2idx, f)\n",
    "        ## 保存词频信息\n",
    "        with open(target_freq_dir, \"w\", encoding=\"utf8\") as f: \n",
    "            json.dump(wordCount, f)\n",
    "                \n",
    "        return word2idx, label2idx\n",
    "    \n",
    "    def _labelToIndex(self, labels, label2idx):\n",
    "        '''\n",
    "        将标签转换为索引表示\n",
    "        '''\n",
    "        try:\n",
    "            labelIds = [label2idx[label] for label in labels]\n",
    "        except:\n",
    "            labelIds = [label2idx[str(label)] for label in labels]\n",
    "        return labelIds\n",
    "    \n",
    "    def _wordToIndex(self, reviews, word2idx):\n",
    "        '''\n",
    "        将词转换为索引表示\n",
    "        '''\n",
    "        reviewIds = [[[word2idx.get(item, word2idx[\"UNK\"]) for item in sent] for sent in doc] for doc in reviews]\n",
    "        return reviewIds\n",
    "    \n",
    "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
    "        '''\n",
    "        生成训练集和验证集\n",
    "        '''\n",
    "        for i, text in enumerate(x):\n",
    "            doc = []\n",
    "            ## 让每个句子的单词数相同\n",
    "            for sent in text:\n",
    "                if len(sent) >= self._sentenceLength:\n",
    "                    doc.append(sent[:self._sentenceLength])\n",
    "                else:\n",
    "                    doc.append(sent + [word2idx[\"PAD\"]] * (self._sentenceLength - len(sent)))\n",
    "            ## 让每篇文章的句子数相同\n",
    "            if len(doc) >= self._docLength:\n",
    "                doc = doc[:self._docLength]\n",
    "            else:\n",
    "                doc.extend([[word2idx[\"PAD\"]] * self._sentenceLength] * (self._docLength - len(doc)))\n",
    "\n",
    "            if i == 0:\n",
    "                reviews = np.array(doc).reshape(1, self._docLength, self._sentenceLength)\n",
    "            else:\n",
    "                reviews = np.concatenate([reviews, np.array(doc).reshape(1, self._docLength, self._sentenceLength)],\n",
    "                                        axis=0)\n",
    "        \n",
    "        \n",
    "        trainIndex = int(len(x)*rate)\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.asarray(y[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.asarray(y[trainIndex:], dtype=\"float32\")\n",
    "        \n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "    \n",
    "    def dataGen(self, path, prefix=\"\"):\n",
    "        '''\n",
    "        path: 表示wordvector所在的文件夹\n",
    "        prefix: 生成单词到索引的文件的前缀\n",
    "        '''\n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        word2idx, label2idx = self._genVocabulary(reviews, labels, path, prefix)\n",
    "        ## 将标签和句子数值化\n",
    "        labelIds = self._labelToIndex(labels, label2idx)\n",
    "        reviewsIds = self._wordToIndex(reviews, word2idx)\n",
    "\n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviewsIds,\n",
    "                                                                                    labelIds,\n",
    "                                                                                    word2idx,\n",
    "                                                                                    self._trainRate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义基本的配置类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:03:06.423557Z",
     "start_time": "2019-08-22T13:03:06.405057Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, path=None):\n",
    "        super().__init__()\n",
    "        ## 定义训练参数\n",
    "        self['num_epochs'] = 5 \n",
    "        self['evaluateEvery'] = 200 \n",
    "        self['checkpointEvery'] = 200 \n",
    "        ### 学习率衰减\n",
    "        self['learningRate'] = 0.05\n",
    "        self['decay_steps'] = 100    ## 学习率衰减的时间点\n",
    "        self['decay_rate'] = 0.9     ## 学习率衰减的幅度\n",
    "        self['grad_clip'] = 5.0  ## 梯度削减\n",
    "        \n",
    "        ## 定义模型参数\n",
    "        self['embeddingSize'] = 200 \n",
    "        self['hiddenSizes'] = 50  ## GRU隐层神经元数\n",
    "        self['dropoutProb'] = 0.5 \n",
    "        self['l2RegLambda'] = 0.001   ## 正则化系数不能过大\n",
    "        \n",
    "        ## 定义基础参数\n",
    "        self['sentenceLength'] = 20   ## 表示每一句话最多的单词数\n",
    "        self['docLength'] = 8    ## 表示每一篇文档最多的句子数\n",
    "        self['batch_size'] = 64 \n",
    "        self['dataSource'] = path  \n",
    "        self['stopWordSource'] = \"../data/english\"\n",
    "        self['numClasses'] = 1 \n",
    "        self['train_size'] = 0.8  ## 训练集和测试集的比例\n",
    "        self.threshold = 0.5 \n",
    "        \n",
    "        ## 保存模型的参数\n",
    "        self['checkpoint_dir'] = \"../model/HAN/yelps/checkpoint\"\n",
    "        self['summary_dir'] = \"../model/HAN/yelps/summary\"\n",
    "        self['max_to_keep'] = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型类和训练类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T11:42:25.133461Z",
     "start_time": "2019-08-22T11:42:25.130094Z"
    }
   },
   "outputs": [],
   "source": [
    "def getSequenceLength(sequences):\n",
    "    '''\n",
    "    sequences: RNN的输入，形状是 [batch, max_time, embed_size]\n",
    "    '''\n",
    "    # 先取绝对值，保证embedding值全部大于等于0\n",
    "    abs_sequences = tf.abs(sequences)\n",
    "    ## 由于padding的向量是全0的，所以最大值也是0\n",
    "    abs_max_seq = tf.reduce_max(abs_sequences, reduction_indices=2)\n",
    "    max_seq_sign = tf.sign(abs_max_seq)\n",
    "    \n",
    "    # 求和就是真实长度\n",
    "    real_len = tf.reduce_sum(max_seq_sign, reduction_indices=1)\n",
    "\n",
    "    return tf.cast(real_len, tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:02:05.348397Z",
     "start_time": "2019-08-22T13:02:05.314863Z"
    }
   },
   "outputs": [],
   "source": [
    "class HAN(BaseModel):\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "        super().__init__(config)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        self.build_model()\n",
    "        self.init_saver()\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        # 输入层\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, self.config['docLength'], self.config['sentenceLength']],\n",
    "                                    name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, self.config['numClasses']], name=\"inputY\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        # embedding层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.embeddings = tf.Variable(self.wordEmbedding, dtype=tf.float32, \n",
    "                                          name=\"wordEmbedding\", trainable=True)\n",
    "            ## 结果 [batch, doc_len, sen_len, embed_size]\n",
    "            self.x0 = tf.nn.embedding_lookup(self.embeddings, self.inputX)\n",
    "        # 首先对句子级别的单词对应编码和Attention\n",
    "        ## 输出[batch*doc_len, hidden_size*2]\n",
    "        self.sen_vec = self.sen2vec(self.x0)\n",
    "        ## 输出[batch, hidden_size*2]\n",
    "        self.doc_vec = self.doc2vec(self.sen_vec)\n",
    "        \n",
    "        with tf.name_scope(\"output\"):\n",
    "            logits = tf.layers.dense(self.doc_vec, self.config['numClasses'], activation=None)\n",
    "            self.predictions = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.inputY, logits=logits)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            if self.config['l2RegLambda'] > 0: \n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) \n",
    "                                    for cand_var in tf.trainable_variables() \n",
    "                                    if \"bia\" not in cand_var.name and \"Embedding\" not in cand_var.name])\n",
    "                self.loss += self.config['l2RegLambda'] * l2_loss\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        \n",
    "        with tf.control_dependencies(update_ops):   \n",
    "            learning_rate = tf.train.exponential_decay(self.config['learningRate'], self.global_step_tensor,\n",
    "                                                      self.config['decay_steps'], \n",
    "                                                      self.config['decay_rate'], staircase=True)\n",
    "          \n",
    "            # 使用梯度削减防止梯度消失或者梯度爆炸\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            \n",
    "        \n",
    "            grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "            for idx, (grad, var) in enumerate(grads_and_vars):\n",
    "                if grad is not None:\n",
    "                    grads_and_vars[idx] = (tf.clip_by_norm(grad, self.config['grad_clip']), var)\n",
    "            \n",
    "            self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step_tensor)\n",
    "        \n",
    "                      \n",
    "    #################################################################################################\n",
    "    \n",
    "    def sen2vec(self, word_embeded):\n",
    "        '''\n",
    "        这里输入word_embeded是[batch, doc_len, sen_len, embed_size]\n",
    "        首先进行句子级别的处理，这时不关注文章的区别，只关注不同的句子，所以将[batch*doc_len]当做长度　\n",
    "        这样符合GRU对输入的要求，最终输出的结果将每个句子的所有单词attention到一个句向量中\n",
    "        '''\n",
    "        with tf.name_scope(\"sen2vec\"):\n",
    "            # 形状[batch_size*doc_len, sen_len, embed_size]\n",
    "            word_embeded = tf.reshape(word_embeded, [-1, self.config['sentenceLength'], self.config['embeddingSize']])\n",
    "            ## 输出 [batch*doc_len, sen_len, hidden_size*2]\n",
    "            word_encoder = self.BidirectionalGRUEncoder(word_embeded, name=\"word_encoder\")\n",
    "            ## 输出 [batch*doc_len, hidden_size*2]\n",
    "            sen_vec = self.AttentionLayer(word_encoder, name=\"word_attention\")\n",
    "            \n",
    "            return sen_vec\n",
    "    \n",
    "    def doc2vec(self, sen_vec):\n",
    "        '''\n",
    "        和sen2vec的操作类似，最后融合成doc向量\n",
    "        '''\n",
    "        with tf.name_scope(\"doc2vec\"):\n",
    "            sen_vec = tf.reshape(sen_vec, [-1, self.config['docLength'], self.config['hiddenSizes']*2])\n",
    "            # shape: [batch, sen_len, hidden_size*2]\n",
    "      \n",
    "            doc_encoder = self.BidirectionalGRUEncoder(sen_vec, name=\"doc_encoder\")\n",
    "            \n",
    "            # shape: [batch, hidden_size*2]\n",
    "            doc_vec = self.AttentionLayer(doc_encoder, name=\"doc_vec\")\n",
    "            return doc_vec\n",
    "    \n",
    "    \n",
    "    def BidirectionalGRUEncoder(self, inputs, name):\n",
    "        '''\n",
    "        双向GRU编码，将一个句子的所有单词或者一个文档的所有句子进行编码得到一个2*hidden_size的输出向量\n",
    "        inputs: [batch, max_time, embedding_size]\n",
    "        outputs: [batch, max_time, 2*hidden_size]\n",
    "        '''\n",
    "\n",
    "        with tf.name_scope(name):\n",
    "            fw_gru_cell = tf.nn.rnn_cell.GRUCell(num_units = self.config['hiddenSizes'])\n",
    "            bw_gru_cell = tf.nn.rnn_cell.GRUCell(num_units = self.config['hiddenSizes'])\n",
    "            fw_gru_cell = tf.nn.rnn_cell.DropoutWrapper(fw_gru_cell, output_keep_prob=self.config['dropoutProb'])\n",
    "            bw_gru_cell = tf.nn.rnn_cell.DropoutWrapper(bw_gru_cell, output_keep_prob=self.config['dropoutProb'])\n",
    "                    \n",
    "            ## fw_outputs和bw_outputs的形状都是 [batch_size, max_time, hidden_size]\n",
    "            (fw_outputs, bw_outputs), (fw_outputs_state, bw_outputs_state) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw=fw_gru_cell, cell_bw=bw_gru_cell, inputs=inputs,\n",
    "                sequence_length=getSequenceLength(inputs), dtype=tf.float32, scope=f\"BiGRU_{name}\")\n",
    "            ## 拼接之后的长度为 [batch_size, max_time, hidden_size*2]          \n",
    "            outputs = tf.concat((fw_outputs, bw_outputs), 2)\n",
    "            \n",
    "            return outputs\n",
    "    \n",
    "    def AttentionLayer(self, inputs, name):\n",
    "        '''\n",
    "        inputs是GRU层的输出\n",
    "        inputs: [batch, max_time, 2*hidden_size]\n",
    "        '''\n",
    "        with tf.name_scope(name):\n",
    "            # context_weight是上下文的重要性向量，用于区分不同单词/句子/文档的重要程度\n",
    "            ## 也就是query\n",
    "            querys = tf.Variable(tf.truncated_normal([self.config['hiddenSizes']*2]), name=\"context_weight\")\n",
    "            \n",
    "            # 使用单层MLP对GRU的输出进行编码\n",
    "            ## 对应key\n",
    "            ### [batch, max_time, 2*hidden_size]\n",
    "            keys = tf.layers.dense(inputs, self.config['hiddenSizes']*2, activation=tf.nn.tanh)\n",
    "            raw_weight = tf.reduce_sum(tf.multiply(keys, querys), axis=2, keepdims=True)\n",
    "            ## shape: [batch, max_time, 1]\n",
    "            alpha = tf.nn.softmax(raw_weight, dim=1)\n",
    "            \n",
    "            ## 得到结果 [batch, 2*hidden_size]\n",
    "            atten_output = tf.reduce_sum(tf.multiply(inputs, alpha), axis=1)\n",
    "            return atten_output\n",
    "    \n",
    "    def init_saver(self):\n",
    "        self.saver = tf.train.Saver(max_to_keep=self.config['max_to_keep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T11:10:12.915052Z",
     "start_time": "2019-08-22T11:10:12.907788Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.length = len(y)\n",
    "        ## 计算不同类别的比例\n",
    "        unique = Counter(self.y.ravel())\n",
    "        self.ratio = [(key, value / self.length) for key, value in unique.items()]\n",
    "        self.indices = []\n",
    "        for key, _ in self.ratio:\n",
    "            index = np.where(y.ravel() == key)\n",
    "            self.indices.append(index)\n",
    "        \n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        '''\n",
    "        生成每一个batch的数据集\n",
    "        '''\n",
    "        choose = np.array([])\n",
    "        for i in range(len(self.indices)):\n",
    "            idx = np.random.choice(self.indices[i][0], \n",
    "                                   max(1, min(len(self.indices[i][0]), int(batch_size*self.ratio[i][1]))))\n",
    "            choose = np.append(choose, idx)\n",
    "        choose = np.random.permutation(choose).astype(\"int64\")\n",
    "        yield self.x[choose], self.y[choose]\n",
    "\n",
    "    def iter_all(self, batch_size):\n",
    "        '''\n",
    "        按照batch迭代所有数据\n",
    "        '''\n",
    "        numBatches = self.length // batch_size + 1\n",
    "        for i in range(numBatches):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, self.length)\n",
    "            batchX = np.array(self.x[start:end], dtype='int64')\n",
    "            batchY = np.array(self.y[start:end], dtype=\"float32\")\n",
    "            yield batchX, batchY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T12:32:00.785581Z",
     "start_time": "2019-08-22T12:32:00.771583Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer(BaseTrain):\n",
    "    def __init__(self, sess, model, data, config, logger):\n",
    "        super(Trainer, self).__init__(sess, model, data, config, logger)\n",
    "        self.train = data[0]\n",
    "        self.eval = data[1]\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        num_iter_per_epoch = self.train.length // self.config[\"batch_size\"]\n",
    "        for _ in tqdm(range(num_iter_per_epoch)):\n",
    "            ## 获取训练结果\n",
    "            loss, metrics, step = self.train_step()\n",
    "            train_acc = metrics['accuracy']\n",
    "            train_f_score = metrics['f_score']\n",
    "            \n",
    "            ## 将训练过程的损失写入\n",
    "            summaries_dict = {\"loss\": loss,\n",
    "                             \"acc\": np.array(train_acc),\n",
    "                             \"f_score\": np.array(train_f_score)}\n",
    "            self.logger.summarize(step, summarizer=\"train\", scope=\"train_summary\",\n",
    "                                 summaries_dict=summaries_dict)\n",
    "            if step % self.config['evaluateEvery'] == 0: \n",
    "                print(\"Train ——　Step: {} | Loss: {} | Acc: {} | F1_Score: {}\".format(\n",
    "                    step, loss, train_acc, train_f_score))\n",
    "                ## 对测试集进行评估\n",
    "                eval_losses = []\n",
    "                eval_predictions = []\n",
    "                eval_true = []\n",
    "                for batchEval in self.eval.iter_all(self.config[\"batch_size\"]):\n",
    "                    loss, predictions = self.eval_step(batchEval[0], batchEval[1])\n",
    "                    eval_losses.append(loss)\n",
    "                    eval_predictions.extend(predictions)\n",
    "                    eval_true.extend(batchEval[-1])\n",
    "                getMetric = Metric( np.array(eval_predictions), np.array(eval_true), self.config)\n",
    "                metrics = getMetric.get_metrics()\n",
    "                eval_prec = np.round(metrics['precision'], 5)\n",
    "                eval_recall = np.round(metrics['recall'], 5)\n",
    "                loss_mean = np.round(np.mean(eval_losses), 5)\n",
    "                \n",
    "                print(\"Evaluation —— Loss: {} | Precision: {} | Recall: {}\".format(\n",
    "                    loss_mean, eval_prec, eval_recall))\n",
    "                summaries_dict = {\"loss\": np.array(loss_mean),\n",
    "                                 \"precision\": np.array(eval_prec),\n",
    "                                 \"recall\": np.array(eval_recall)}\n",
    "                self.logger.summarize(step, summarizer=\"test\", scope=\"test_summary\",\n",
    "                                     summaries_dict=summaries_dict)\n",
    "            \n",
    "            if step % self.config['checkpointEvery'] == 0:\n",
    "                self.model.save(self.sess)\n",
    "                    \n",
    "    \n",
    "    def train_step(self):\n",
    "        batch_x, batch_y = next(self.train.next_batch(self.config[\"batch_size\"]))\n",
    "        feed_dict = {self.model.inputX: batch_x,\n",
    "                    self.model.inputY: batch_y, \n",
    "                    self.model.dropout_keep_prob: self.config['dropoutProb']}\n",
    "        _, loss, predictions, step = self.sess.run([self.model.train_op,\n",
    "                                                   self.model.loss,\n",
    "                                                   self.model.predictions, \n",
    "                                                   self.model.global_step_tensor],\n",
    "                                                  feed_dict=feed_dict)\n",
    "        getMetric = Metric(predictions, batch_y, self.config)\n",
    "        metrics = getMetric.get_metrics()\n",
    "        return loss, metrics, step \n",
    "    \n",
    "    def eval_step(self, batch_x, batch_y):\n",
    "        feed_dict = {self.model.inputX: batch_x,\n",
    "                    self.model.inputY:batch_y,\n",
    "                    self.model.dropout_keep_prob: 1.0}\n",
    "        loss, predictions = self.sess.run([self.model.loss, self.model.predictions],\n",
    "                                         feed_dict=feed_dict)\n",
    "\n",
    "        return loss, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用yelps数据集训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:03:21.654916Z",
     "start_time": "2019-08-22T13:03:21.651356Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T11:47:03.302407Z",
     "start_time": "2019-08-22T11:42:57.194922Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"../data/yelps/yelps.csv\"\n",
    "config = Config(path)\n",
    "\n",
    "create_dirs([config['summary_dir'], config['checkpoint_dir']])\n",
    "\n",
    "data = Dataset(config)\n",
    "data.dataGen(\"../data/yelps/\", prefix=\"yelps\")\n",
    "\n",
    "train_X, train_y, eval_X, eval_y = data.trainReviews, data.trainLabels, data.evalReviews, data.evalLabels\n",
    "wordEmbedding, labels = data.wordEmbedding, data.labelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:05:49.157275Z",
     "start_time": "2019-08-22T13:05:49.140053Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = DataGenerator(train_X, train_y)\n",
    "eval_data = DataGenerator(eval_X, eval_y)\n",
    "pack_data = [train_data, eval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:12:29.229987Z",
     "start_time": "2019-08-22T13:06:03.387996Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前正处于第1次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d16b045c81e4b24ab035a5583d1c343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ——　Step: 200 | Loss: 0.6856454610824585 | Acc: 0.84127 | F1_Score: 0.87179\n",
      "Evaluation —— Loss: 0.6800900101661682 | Precision: 0.88695 | Recall: 0.83744\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 400 | Loss: 0.4370688498020172 | Acc: 0.88889 | F1_Score: 0.91954\n",
      "Evaluation —— Loss: 0.5730000138282776 | Precision: 0.92467 | Recall: 0.77149\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 600 | Loss: 0.5032904744148254 | Acc: 0.8254 | F1_Score: 0.88172\n",
      "Evaluation —— Loss: 0.44317999482154846 | Precision: 0.88223 | Recall: 0.92129\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 800 | Loss: 0.47491201758384705 | Acc: 0.90476 | F1_Score: 0.93182\n",
      "Evaluation —— Loss: 0.4517799913883209 | Precision: 0.8657 | Recall: 0.94221\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 1000 | Loss: 0.28165656328201294 | Acc: 0.90476 | F1_Score: 0.92857\n",
      "Evaluation —— Loss: 0.42076000571250916 | Precision: 0.87313 | Recall: 0.93269\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 1200 | Loss: 0.4833293557167053 | Acc: 0.79365 | F1_Score: 0.84706\n",
      "Evaluation —— Loss: 0.43957000970840454 | Precision: 0.91136 | Recall: 0.85821\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "当前正处于第2次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458facd56e324f61b85eec317b32ef12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ——　Step: 1400 | Loss: 0.31217390298843384 | Acc: 0.85714 | F1_Score: 0.89157\n",
      "Evaluation —— Loss: 0.4099099934101105 | Precision: 0.92464 | Recall: 0.84159\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 1600 | Loss: 0.37229737639427185 | Acc: 0.85714 | F1_Score: 0.89412\n",
      "Evaluation —— Loss: 0.36840999126434326 | Precision: 0.90174 | Recall: 0.90399\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 1800 | Loss: 0.3807463049888611 | Acc: 0.85714 | F1_Score: 0.89412\n",
      "Evaluation —— Loss: 0.38005998730659485 | Precision: 0.86114 | Recall: 0.95007\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 2000 | Loss: 0.3118847608566284 | Acc: 0.88889 | F1_Score: 0.91358\n",
      "Evaluation —— Loss: 0.3496200144290924 | Precision: 0.89633 | Recall: 0.91373\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 2200 | Loss: 0.2534031867980957 | Acc: 0.88889 | F1_Score: 0.91765\n",
      "Evaluation —— Loss: 0.3443799912929535 | Precision: 0.90405 | Recall: 0.90678\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 2400 | Loss: 0.2383711338043213 | Acc: 0.93651 | F1_Score: 0.95455\n",
      "Evaluation —— Loss: 0.3499999940395355 | Precision: 0.89685 | Recall: 0.91819\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "当前正处于第3次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b0398e960a47baaaed49266163a798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ——　Step: 2600 | Loss: 0.2147609293460846 | Acc: 0.96825 | F1_Score: 0.97619\n",
      "Evaluation —— Loss: 0.3557400107383728 | Precision: 0.91099 | Recall: 0.88525\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 2800 | Loss: 0.15098153054714203 | Acc: 0.96825 | F1_Score: 0.97619\n",
      "Evaluation —— Loss: 0.3401699960231781 | Precision: 0.91177 | Recall: 0.89462\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 3000 | Loss: 0.17149394750595093 | Acc: 0.93651 | F1_Score: 0.95\n",
      "Evaluation —— Loss: 0.34046998620033264 | Precision: 0.88844 | Recall: 0.93126\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 3200 | Loss: 0.17516827583312988 | Acc: 0.95238 | F1_Score: 0.96386\n",
      "Evaluation —— Loss: 0.35416001081466675 | Precision: 0.87971 | Recall: 0.94191\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 3400 | Loss: 0.23619872331619263 | Acc: 0.93651 | F1_Score: 0.95349\n",
      "Evaluation —— Loss: 0.34321001172065735 | Precision: 0.89132 | Recall: 0.9237\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 3600 | Loss: 0.16236452758312225 | Acc: 0.96825 | F1_Score: 0.97674\n",
      "Evaluation —— Loss: 0.3421599864959717 | Precision: 0.89096 | Recall: 0.9259\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "当前正处于第4次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864604daaa264c9e8e0c22dd2a4ae4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ——　Step: 3800 | Loss: 0.14993001520633698 | Acc: 0.93651 | F1_Score: 0.95349\n",
      "Evaluation —— Loss: 0.3515099883079529 | Precision: 0.89905 | Recall: 0.91698\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 4000 | Loss: 0.16456782817840576 | Acc: 0.93651 | F1_Score: 0.95349\n",
      "Evaluation —— Loss: 0.34007999300956726 | Precision: 0.90189 | Recall: 0.9132\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 4200 | Loss: 0.1182461678981781 | Acc: 0.96825 | F1_Score: 0.97619\n",
      "Evaluation —— Loss: 0.3443700075149536 | Precision: 0.88816 | Recall: 0.92741\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 4400 | Loss: 0.20902268588542938 | Acc: 0.92063 | F1_Score: 0.94118\n",
      "Evaluation —— Loss: 0.34338000416755676 | Precision: 0.88695 | Recall: 0.93164\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 4600 | Loss: 0.10863339155912399 | Acc: 0.96825 | F1_Score: 0.97619\n",
      "Evaluation —— Loss: 0.3399200141429901 | Precision: 0.89927 | Recall: 0.91517\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 4800 | Loss: 0.13014714419841766 | Acc: 0.96825 | F1_Score: 0.97674\n",
      "Evaluation —— Loss: 0.3488200008869171 | Precision: 0.8946 | Recall: 0.92197\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 5000 | Loss: 0.1854686290025711 | Acc: 0.92063 | F1_Score: 0.93976\n",
      "Evaluation —— Loss: 0.3502100110054016 | Precision: 0.89581 | Recall: 0.91834\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "当前正处于第5次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ea2e6668b446d2a83ff85f9645d273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ——　Step: 5200 | Loss: 0.31845641136169434 | Acc: 0.87302 | F1_Score: 0.90909\n",
      "Evaluation —— Loss: 0.34318000078201294 | Precision: 0.88936 | Recall: 0.92907\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 5400 | Loss: 0.21163500845432281 | Acc: 0.92063 | F1_Score: 0.94253\n",
      "Evaluation —— Loss: 0.34981000423431396 | Precision: 0.89963 | Recall: 0.91404\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 5600 | Loss: 0.17650136351585388 | Acc: 0.93651 | F1_Score: 0.95238\n",
      "Evaluation —— Loss: 0.35097000002861023 | Precision: 0.90397 | Recall: 0.9052\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 5800 | Loss: 0.1434902399778366 | Acc: 0.96825 | F1_Score: 0.97674\n",
      "Evaluation —— Loss: 0.3462899923324585 | Precision: 0.89733 | Recall: 0.91638\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 6000 | Loss: 0.27254602313041687 | Acc: 0.88889 | F1_Score: 0.91566\n",
      "Evaluation —— Loss: 0.3505699932575226 | Precision: 0.89652 | Recall: 0.91562\n",
      "Saving model...\n",
      "Model saved\n",
      "Train ——　Step: 6200 | Loss: 0.17915242910385132 | Acc: 0.93651 | F1_Score: 0.95455\n",
      "Evaluation —— Loss: 0.3488999903202057 | Precision: 0.89713 | Recall: 0.91774\n",
      "Saving model...\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "config['num_epochs'] = 5\n",
    "\n",
    "create_dirs([config['summary_dir'], config['checkpoint_dir']])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "## 设置计算图的配置\n",
    "session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "session_conf.gpu_options.allow_growth = True\n",
    "session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9 \n",
    "\n",
    "sess = tf.Session(config=session_conf)\n",
    "\n",
    "model = HAN(config, wordEmbedding)\n",
    "\n",
    "logger = Logger(sess, config)\n",
    "\n",
    "trainer = Trainer(sess, model, pack_data, config, logger)\n",
    "trainer.train_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证集最终结果为　——　Precision: 0.89713, Recall: 0.91774"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个实验过程中花费了比较长的时间，做一下记录：开始模型的梯度总是会变成全０，导致输出一直有问题，本来以为是模型的问题（也确实有几个敲错的地方），更正了敲错的地方之后仍然存在问题。多次调试之后发现了原因：\n",
    "\n",
    "- **L2正则化系数过大，一般设置不要超过$10^{-3}$级别，当然也不要太小**；\n",
    "\n",
    "- **学习率衰减的步数太大，由于本次使用的数据集比较小，所以衰减步数太大导致收敛太慢**。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
