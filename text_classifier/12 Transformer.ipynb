{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import json \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf \n",
    "from collections import Counter\n",
    "from tqdm.autonotebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 定义基础的配置类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, path=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 定义训练参数\n",
    "        self['num_epochs'] = 2 \n",
    "        self['batch_size'] = 64\n",
    "        self['sequenceLength'] = 200\n",
    "        self['evaluateEvery'] = 100 \n",
    "        self['checkpointEvery'] = 100 \n",
    "        \n",
    "        # 学习率衰减\n",
    "        self['learningRate'] = 0.01 \n",
    "        self['decay_steps'] = 100   # 学习率每隔多少个step衰减一次\n",
    "        self['decay_rate'] = 0.9    # 学习率每次衰减的比例\n",
    "        self['grad_clip'] = 4.0     # 梯度削减的系数\n",
    "        \n",
    "        # 定义模型参数\n",
    "        self['embeddingSize'] = 200 \n",
    "        self[\"filters\"] = 128     # 内层一维卷积核的数量，外层卷积核的数量要等于embeddingSize，因为要shorcut\n",
    "        self['numHeads'] = 8      # Attention中heads的数量\n",
    "        self['numBlocks'] = 1     # 设置Transformer中block的数量\n",
    "        self['epsilon'] =  1e-8   # LayerNorm中最小的除数\n",
    "        self['attention_keepProb'] = 0.9  # multi-head attention中的dropout\n",
    "        self['dropoutProb'] = 0.5  # 全连接层的dropout\n",
    "        self['l2RegLambda'] = 0.000 \n",
    "        \n",
    "        # 设置基础参数\n",
    "        self['dataSource'] = path\n",
    "        self['stopWordSource'] = \"../data/english\"\n",
    "        self['numClasses'] = 1 \n",
    "        self['train_size'] = 0.8   # 训练集所占的比例\n",
    "        self.threshold = 0.5 \n",
    "        \n",
    "        # 保存模型的参数\n",
    "        self['checkpoint_dir'] = \"../model/Transformer/imdb/checkpoint\"\n",
    "        self['summary_dir'] = \"../model/Transformer/imdb/summary\"\n",
    "        self['max_to_keep'] = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 定义模型类和训练类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 定义模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siusoid_encoding_table(config, padding_idx=0):\n",
    "    '''\n",
    "    n_position: 表示总共的位置的数量，也就是序列的长度，sequenceLength\n",
    "    d_hid: 表示位置编码的神经元数，和词向量的size相同\n",
    "    padding_idx：表示pad的索引\n",
    "    '''\n",
    "    n_position = config['sequenceLength'] \n",
    "    d_hid = config['embeddingSize']\n",
    "    # 计算某一个位置向量不同位置的值\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2*(hid_idx//2)/d_hid)\n",
    "    # 获取某个特定位置的词向量角度值\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_hid)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position+1)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # 对应dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # 对应dim 2i+1\n",
    "    \n",
    "    if padding_idx is not None:\n",
    "        sinusoid_table[padding_idx] = 0.\n",
    "        \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_input(batch_x, config):\n",
    "    '''\n",
    "    batch_x: 形状为[batch, seq_len]\n",
    "    '''\n",
    "    # 计算每一个句子的长度，得到维度为1的ndarray表示每个句子实际长度\n",
    "    actual_len = np.sum(~np.equal(batch_x, 0), axis=1)\n",
    "    ## 得到每个位置的标号，维度[batch, seq_len]\n",
    "    position = list(map(lambda l: list(range(1, l+1))+[0]*(config['sequenceLength']-l), actual_len))\n",
    "    return np.array(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transfomer(BaseModel):\n",
    "    def __init__(self, config, wordEmbedding, posEmbedding):\n",
    "        super().__init__(config)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        self.posEmbedding = posEmbedding\n",
    "        self.build_model()\n",
    "        self.init_saver()\n",
    "\n",
    "    def build_model(self):\n",
    "        # 输入层\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, self.config['sequenceLength']], name=\"inputX\")\n",
    "        self.inputPos = tf.placeholder(tf.int32, [None, self.config['sequenceLength']], name=\"inputPos\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None], name=\"inputY\")\n",
    "\n",
    "        self.atten_keep_prob = tf.placeholder(tf.float32, name=\"atten_keep_prob\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"doprout_keep_prob\")\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.word_embeded = tf.nn.embedding_lookup(self.word_embeddings, self.inputX)\n",
    "            self.pos_embeded = tf.nn.embedding_lookup(self.pos_embeddings, self.inputPos)\n",
    "            ## 得到维度 [batch, seq_len, embed_size]\n",
    "            self.embeded = tf.add(self.word_embeded, self.pos_embeded)\n",
    "\n",
    "        with tf.name_scope(\"transformer\"):\n",
    "            ## 对于不同的block\n",
    "            for i in range(self.config[\"numBlocks\"]):\n",
    "                with tf.name_scope(f\"block_{i}\"):\n",
    "                    # 得到维度 [batch, seq_len, embed_size]\n",
    "                    multiHeadAtten = self._multiheadAttention(rawKeys=self.inputX,\n",
    "                                                             queries=self.embeded,\n",
    "                                                             keys=self.embeded)\n",
    "                    self.embeded = self._feedForward(multiHeadAtten,\n",
    "                                                         [self.config['filters'], self.config['embeddingSize']])\n",
    "            outputs = tf.reshape(self.embeded, \n",
    "                                 [-1, self.config['sequenceLength']*self.config['embeddingSize']])\n",
    "        outputSize = outputs.get_shape().as_list()[-1]\n",
    "\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            outputs = tf.nn.dropout(outputs, keep_prob=self.dropout_keep_prob)\n",
    "\n",
    "        # 全连接层输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\"outputW\", \n",
    "                                     shape=[outputSize, self.config['numClasses']],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            outputB = tf.Variable(tf.constant(0.1, shape=[self.config['numClasses']]), name=\"outputB\")\n",
    "            l2Loss = tf.nn.l2_loss(outputW)\n",
    "            self.logits = tf.add(tf.matmul(outputs, outputW), outputB, name=\"logits\")\n",
    "\n",
    "        if self.config['numClasses'] == 1:\n",
    "            self.predictions = tf.nn.sigmoid(self.logits)\n",
    "        elif self.config['numClasses'] > 1:\n",
    "            self.predictions = tf.nn.softmax(self.logits, axis=-1)\n",
    "\n",
    "        # 计算损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(tf.reshape(self.inputY, [-1, 1]),\n",
    "                                                                           dtype=tf.float32),\n",
    "                                                            logits=self.logits)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            if self.config['l2RegLambda'] > 0: \n",
    "                self.loss += self.config['l2RegLambda'] * l2Loss\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            learning_rate = tf.train.exponential_decay(self.config['learningRate'],\n",
    "                                                      self.global_step_tensor, \n",
    "                                                      self.config['decay_steps'],\n",
    "                                                      self.config['decay_rate'],\n",
    "                                                      staircase=True)\n",
    "            ## 使用梯度削减防止梯度爆炸\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "            grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "            for idx, (grad, var) in enumerate(grads_and_vars):\n",
    "                if grad is not None:\n",
    "                    grads_and_vars[idx] = (tf.clip_by_norm(grad, self.config['grad_clip']), var)\n",
    "            self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step_tensor)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        with tf.name_scope(\"weights\"):\n",
    "            self.word_embeddings = tf.Variable(tf.cast(self.wordEmbedding, dtype=tf.float32),\n",
    "                                              name=\"wordEmbedding\", trainable=False)\n",
    "            self.pos_embeddings = tf.Variable(tf.cast(self.posEmbedding, dtype=tf.float32),\n",
    "                                             name=\"posEmbedding\", trainable=False)\n",
    "\n",
    "    def _multiheadAttention(self, rawKeys, queries, keys, numUnits=None, causality=False,\n",
    "                           scope=\"multi-headAttention\"):\n",
    "        numHeads = self.config['numHeads']\n",
    "\n",
    "        ## 如果没有传入多个heads合在一起的神经元数，则直接用emedding_size的数量\n",
    "        if numUnits is None:\n",
    "            numUnits = queries.get_shape().as_list()[-1]\n",
    "\n",
    "        with tf.name_scope(scope):\n",
    "            ## 将值进行非线性映射，得到多个head的神经元值\n",
    "            Q = tf.layers.dense(queries, numUnits, activation=tf.nn.relu)\n",
    "            K = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "            V = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "\n",
    "            ## 将数据按照最后一维分割成num_heads个，然后按照第一维拼接\n",
    "            ##得到新的Q, K, V的维度为 [batch_size*numHeads, seq_len, embed_size/numHeads]\n",
    "            Q_ = tf.concat(tf.split(Q, numHeads, axis=-1), axis=0)\n",
    "            K_ = tf.concat(tf.split(K, numHeads, axis=-1), axis=0)\n",
    "            V_ = tf.concat(tf.split(V, numHeads, axis=-1), axis=0)\n",
    "\n",
    "            ## 计算query和key之间的点积，得到维度[batch*heads, seq_len, seq_len]\n",
    "            similary = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))\n",
    "\n",
    "            ## 对计算的点击进行scaled\n",
    "            scaledSimilarity = similary / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "            ## 对padding进行mask，第二个参数表示每个维度扩充的数量，得到[batch*heads, seq_len]\n",
    "            keyMasks = tf.tile(rawKeys, [numHeads, 1])\n",
    "            ## 增加一个维度，并扩充得到结果 [batch*heads, seq_len, seq_len]\n",
    "            keyMasks = tf.tile(tf.expand_dims(keyMasks, 1), [1, tf.shape(queries)[1], 1])\n",
    "            ## 生成全1矩阵，维度和scaledSimilarity相同，然后得到负无穷\n",
    "            paddings = tf.ones_like(scaledSimilarity) * (-np.inf)\n",
    "\n",
    "            ## tf.where(condition, x, y)，其中condition元素为bool值，对应True用x中元素替换，False用y中元素替换\n",
    "            ## 也就是说，下面的效果就是等于0的位置用-inf替换，维度为 [batch*heads, seq_len, seq_len]\n",
    "            maskedSimilarity = tf.where(tf.equal(keyMasks, 0), paddings, scaledSimilarity)\n",
    "\n",
    "            # 如果是Decoder，需要将当前单词后面的部分mask掉\n",
    "            if causality:\n",
    "                ## 得到维度 [seq_len, seq_len]\n",
    "                diagVals = tf.ones_like(maskedSimilarity[0, :, :])\n",
    "                ## 生成下三角，维度[seq_len, seq_len]\n",
    "                tril = tf.linalg.LinearOperatorLowerTriangular(diagVals).to_dense()\n",
    "                masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(maskedSimilarity)[0], 1, 1])\n",
    "                paddings = tf.ones_like(masks) * (-np.inf)\n",
    "                maskedSimilarity = tf.where(tf.equal(masks, 0), paddings, maskedSimilarity)\n",
    "\n",
    "            # 通过softmax计算加权系数\n",
    "            weights = tf.nn.softmax(maskedSimilarity)\n",
    "            ##加权和得到输出\n",
    "            outputs = tf.matmul(weights, V_)\n",
    "            ## 将多头Attention计算得到的输出进行维度重组，得到[batch_size, seq_len, embed_size]\n",
    "            outputs = tf.concat(tf.split(outputs, numHeads, axis=0), axis=2)\n",
    "            outputs = tf.nn.dropout(outputs, self.atten_keep_prob)\n",
    "\n",
    "            # 对每个subLayers建立残差连接\n",
    "            outputs = tf.add(outputs, queries)\n",
    "            ## layerNormalization\n",
    "            outputs = self._layerNormalization(outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _layerNormalization(self, inputs, scope=\"layerNorm\"):\n",
    "        with tf.name_scope(scope):\n",
    "            # 获取输入的维度，[batch, seq_len, embed_size]\n",
    "            inputsShape = inputs.get_shape()\n",
    "            paramsShape = inputsShape[-1:]\n",
    "\n",
    "            ## LayerNormalization考虑在最后一个维度上计算数据的均值和方差\n",
    "            ## mean和variance的维度都是[batch, seq_len, 1]\n",
    "            mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "            beta = tf.Variable(tf.zeros(paramsShape))\n",
    "            gamma = tf.Variable(tf.ones(paramsShape))\n",
    "            normalized = (inputs - mean) / ((variance + self.config['epsilon'])** 0.5)\n",
    "            outputs = gamma * normalized + beta\n",
    "            return outputs\n",
    "\n",
    "    def _feedForward(self, inputs, filters, scope=\"feedForward\"):\n",
    "        # 前向传播采用一维卷积神经网络\n",
    "        with tf.name_scope(scope):\n",
    "            ## 内层\n",
    "            params = {\"inputs\": inputs, \"filters\": filters[0], \"kernel_size\": 1,\n",
    "                     \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "\n",
    "            outputs = tf.layers.conv1d(**params)\n",
    "\n",
    "            ## 外层\n",
    "            params = {\"inputs\": outputs, \"filters\": filters[1], \"kernel_size\": 1,\n",
    "                     \"activation\": None, \"use_bias\": True}\n",
    "            outputs = tf.layers.conv1d(**params)\n",
    "\n",
    "            ## 残差连接\n",
    "            outputs += inputs \n",
    "\n",
    "            ## LayerNormalization\n",
    "            outputs = self._layerNormalization(outputs)\n",
    "\n",
    "            return outputs\n",
    "    \n",
    "    def init_saver(self):\n",
    "        self.saver = tf.train.Saver(max_to_keep=self.config['max_to_keep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 定义训练类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(BaseTrain):\n",
    "    def __init__(self, sess, model, data, config, logger):\n",
    "        super().__init__(sess, model, data, config, logger)\n",
    "        self.train = data[0]\n",
    "        self.eval = data[1]\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        num_iter_per_epoch = self.train.length // self.config['batch_size']\n",
    "        for i in tqdm(range(num_iter_per_epoch)):\n",
    "            ## 获取训练结果\n",
    "            loss, metrics, step = self.train_step()\n",
    "            train_acc = metrics['accuracy']\n",
    "            train_f_score = metrics['f_score']\n",
    "            \n",
    "            # 将训练过程的损失写入\n",
    "            summaries_dict = {\"loss\": loss, \n",
    "                             \"acc\": np.array(train_acc),\n",
    "                             \"f_score\": np.array(train_f_score)}\n",
    "            self.logger.summarize(step, summarizer=\"train\", scope=\"train_summary\",\n",
    "                                 summaries_dict=summaries_dict)\n",
    "            if step % self.config['evaluateEvery'] == 0: \n",
    "                print(\"Train —— Step: {} | Loss: {} | Acc: {} : F1_Score: {}\".format(\n",
    "                    step, loss, train_acc, train_f_score))\n",
    "                # 对测试集进行评估\n",
    "                eval_losses = []\n",
    "                eval_pred = []\n",
    "                eval_true = []\n",
    "                for batchEval in self.eval.iter_all(self.config['batch_size']):\n",
    "                    loss, predictions = self.eval_step(batchEval[0], batchEval[1])\n",
    "                    eval_losses.append(loss)\n",
    "                    eval_pred.extend(predictions)\n",
    "                    eval_true.extend(batchEval[-1])\n",
    "                getMetric = Metric(np.array(eval_pred), np.array(eval_true),\n",
    "                                  self.config)\n",
    "                metrics = getMetric.get_metrics()\n",
    "                eval_prec = np.round(metrics['precision'], 5)\n",
    "                eval_recall = np.round(metrics['recall'], 5)\n",
    "                loss_mean = np.round(np.mean(eval_losses), 5)\n",
    "                print(\"Evaluation —— Loss: {} | Precision: {} | Recall: {}\".format(\n",
    "                    loss_mean, eval_prec, eval_recall))\n",
    "                summaries_dict = {\"loss\": np.array(loss_mean),\n",
    "                                 \"precision\": np.array(eval_prec), \n",
    "                                 \"recall\": np.array(eval_recall)}\n",
    "                self.logger.summarize(step, summarizer=\"test\", scope=\"test_summary\",\n",
    "                                     summaries_dict=summaries_dict)\n",
    "            if step % self.config['checkpointEvery'] == 0: \n",
    "                self.model.save(self.sess)\n",
    "            \n",
    "            \n",
    "    def train_step(self):\n",
    "        batch_x, batch_y = next(self.train.next_batch(self.config['batch_size']))\n",
    "        batch_pos = get_position_input(batch_x, self.config)\n",
    "        feed_dict = {self.model.inputX: batch_x, \n",
    "                    self.model.inputPos: batch_pos,\n",
    "                    self.model.inputY: batch_y,\n",
    "                    self.model.dropout_keep_prob: self.config['dropoutProb'],\n",
    "                    self.model.atten_keep_prob: self.config['attention_keepProb']}\n",
    "        _, loss, predictions, step = self.sess.run([self.model.train_op,\n",
    "                                                   self.model.loss,\n",
    "                                                   self.model.predictions, \n",
    "                                                   self.model.global_step_tensor],\n",
    "                                                  feed_dict=feed_dict)\n",
    "        getMetric = Metric(predictions, batch_y, self.config)\n",
    "        metrics = getMetric.get_metrics()\n",
    "        return loss, metrics, step\n",
    "    \n",
    "    def eval_step(self, batch_x, batch_y):\n",
    "        batch_pos = get_position_input(batch_x, self.config)\n",
    "        feed_dict = {self.model.inputX: batch_x,\n",
    "                    self.model.inputPos: batch_pos,\n",
    "                    self.model.inputY: batch_y,\n",
    "                    self.model.dropout_keep_prob: 1.0,\n",
    "                    self.model.atten_keep_prob: 1.0}\n",
    "        loss, predictions = self.sess.run([self.model.loss, self.model.predictions],\n",
    "                                         feed_dict=feed_dict)\n",
    "        return loss, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 使用数据进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 使用IMDB数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    path = \"../data/imdb/labeldTrain.csv\"\n",
    "    config = Config(path)\n",
    "    create_dirs([config[\"summary_dir\"], config['checkpoint_dir']])\n",
    "    data = Dataset(config)\n",
    "    \n",
    "    ## 生成训练集数据，第一个参数表示wordEmbedding文件所在的文件夹\n",
    "    data.dataGen(\"../data/imdb\", prefix=\"imdb\")\n",
    "    \n",
    "    train_X, train_y, eval_X, eval_y = data.trainReviews, data.trainLabels, data.evalReviews,data.evalLabels\n",
    "    wordEmbedding, labels = data.wordEmbedding, data.labelList\n",
    "    posEmbedding = get_siusoid_encoding_table(config)\n",
    "    \n",
    "    #print(train_X.shape)\n",
    "    #print(train_y.shape)\n",
    "    #print(posEmbedding.shape)\n",
    "    \n",
    "    train_data = DataGenerator(train_X, train_y)\n",
    "    eval_data = DataGenerator(eval_X, eval_y)\n",
    "    pack_data = [train_data, eval_data]\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    ## 设置计算图的配置\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9 \n",
    "    \n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 创建一个模型\n",
    "    model = Transfomer(config, wordEmbedding, posEmbedding)\n",
    "    logger = Logger(sess, config)\n",
    "    \n",
    "    trainer = Trainer(sess, model, pack_data, config, logger)\n",
    "    trainer.train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前正处于第1次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fc038e37c8402f811245a27250ab7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 100 | Loss: 1.3383203744888306 | Acc: 0.53968 : F1_Score: 0.61333\n",
      "Evaluation —— Loss: 1.3322299718856812 | Precision: 0.0 | Recall: 0.0\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 200 | Loss: 0.742250919342041 | Acc: 0.52381 : F1_Score: 0.28571\n",
      "Evaluation —— Loss: 0.7258599996566772 | Precision: 0.50673 | Recall: 0.99723\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 300 | Loss: 0.7877440452575684 | Acc: 0.39683 : F1_Score: 0.32143\n",
      "Evaluation —— Loss: 0.6904000043869019 | Precision: 0.51759 | Recall: 0.98339\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "当前正处于第2次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1b59e5d2914554af7b330e9bd39217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 400 | Loss: 0.6745485663414001 | Acc: 0.61905 : F1_Score: 0.42857\n",
      "Evaluation —— Loss: 0.5920699834823608 | Precision: 0.62135 | Recall: 0.96915\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 500 | Loss: 0.45134687423706055 | Acc: 0.77778 : F1_Score: 0.76667\n",
      "Evaluation —— Loss: 0.41802000999450684 | Precision: 0.79611 | Recall: 0.84177\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 600 | Loss: 0.3355503976345062 | Acc: 0.87302 : F1_Score: 0.85185\n",
      "Evaluation —— Loss: 0.4621100127696991 | Precision: 0.7388 | Recall: 0.92642\n",
      "Saving model...\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 使用Yelps数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    path = \"../data/yelps/yelps.csv\"\n",
    "    config = Config(path)\n",
    "    config['summary_dir'] = \"../model/Transformer/yelps/summary\"\n",
    "    config['checkpoint_dir'] = \"../model/Transformer/yelps/checkpoint\"\n",
    "    config['evaluateEvery'] = 400\n",
    "    config['checkpointEvery'] = 400\n",
    "    create_dirs([config[\"summary_dir\"], config['checkpoint_dir']])\n",
    "    data = Dataset(config)\n",
    "    \n",
    "    ## 生成训练集数据，第一个参数表示wordEmbedding文件所在的文件夹\n",
    "    data.dataGen(\"../data/yelps\", prefix=\"yelps\")\n",
    "    \n",
    "    train_X, train_y, eval_X, eval_y = data.trainReviews, data.trainLabels, data.evalReviews,data.evalLabels\n",
    "    wordEmbedding, labels = data.wordEmbedding, data.labelList\n",
    "    posEmbedding = get_siusoid_encoding_table(config)\n",
    "    \n",
    "    #print(train_X.shape)\n",
    "    #print(train_y.shape)\n",
    "    #print(posEmbedding.shape)\n",
    "    \n",
    "    train_data = DataGenerator(train_X, train_y)\n",
    "    eval_data = DataGenerator(eval_X, eval_y)\n",
    "    pack_data = [train_data, eval_data]\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    ## 设置计算图的配置\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9 \n",
    "    \n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 创建一个模型\n",
    "    model = Transfomer(config, wordEmbedding, posEmbedding)\n",
    "    logger = Logger(sess, config)\n",
    "    \n",
    "    trainer = Trainer(sess, model, pack_data, config, logger)\n",
    "    trainer.train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前正处于第1次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb3f930f9074f00aa2be02b6ee39627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 400 | Loss: 0.7525515556335449 | Acc: 0.46032 : F1_Score: 0.55263\n",
      "Evaluation —— Loss: 0.6349300146102905 | Precision: 0.70607 | Recall: 0.81712\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 800 | Loss: 0.3563399016857147 | Acc: 0.84127 : F1_Score: 0.875\n",
      "Evaluation —— Loss: 0.3035399913787842 | Precision: 0.87832 | Recall: 0.93405\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1200 | Loss: 0.300914466381073 | Acc: 0.90476 : F1_Score: 0.93023\n",
      "Evaluation —— Loss: 0.28415998816490173 | Precision: 0.88684 | Recall: 0.9401\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "当前正处于第2次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f30467541ce4dbfbdc3220c3341d2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 1600 | Loss: 0.4448325037956238 | Acc: 0.80952 : F1_Score: 0.84615\n",
      "Evaluation —— Loss: 0.2841300070285797 | Precision: 0.89375 | Recall: 0.93602\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 2000 | Loss: 0.2741104066371918 | Acc: 0.84127 : F1_Score: 0.88372\n",
      "Evaluation —— Loss: 0.2786099910736084 | Precision: 0.91397 | Recall: 0.90119\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 2400 | Loss: 0.16847646236419678 | Acc: 0.88889 : F1_Score: 0.91954\n",
      "Evaluation —— Loss: 0.26903000473976135 | Precision: 0.90682 | Recall: 0.92114\n",
      "Saving model...\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论：在yelps数据集下能够得到还不错的效果，在IMDB数据集下效果相对一般；使用Transformer进行文本分类，超参数调节是一个比较重要的点，不然会出现梯度消失的情况，关键点是block不能太多，因为文本分类是一个比较简单的任务，不需要过于复杂的网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
