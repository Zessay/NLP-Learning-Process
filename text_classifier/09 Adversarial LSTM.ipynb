{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T09:12:52.048285Z",
     "start_time": "2019-08-23T09:12:52.040211Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import time \n",
    "import datetime \n",
    "import json \n",
    "import math \n",
    "import logging \n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm.autonotebook import tqdm\n",
    "from collections import Counter\n",
    "import gensim \n",
    "import tensorflow as tf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T09:11:22.608724Z",
     "start_time": "2019-08-23T09:11:13.775517Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义配置类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本配置类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T07:16:38.664642Z",
     "start_time": "2019-08-22T07:16:38.659976Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, path=None):\n",
    "        super().__init__()\n",
    "        ## 定义训练参数\n",
    "        self['num_epochs'] = 5 \n",
    "        self['evaluateEvery'] = 100 \n",
    "        self['checkpointEvery'] = 100 \n",
    "        self['learningRate'] = 0.001 \n",
    "        \n",
    "        ## 定义模型参数\n",
    "        self['embeddingSize'] = 200 \n",
    "        self['hiddenSizes'] = 128   ## LSTM网络神经元个数\n",
    "        self['dropoutProb'] = 0.5  \n",
    "        self['l2RegLambda'] = 0.0 \n",
    "        self['epsilon'] = 5 \n",
    "        \n",
    "        ## 定义基础参数\n",
    "        self['sequenceLength'] = 200 \n",
    "        self['batch_size'] = 64 \n",
    "        self['dataSource'] = path \n",
    "        self['stopWordSource'] = \"../data/english\"\n",
    "        self['numClasses'] = 1 \n",
    "        self['train_size'] = 0.8   ## 训练集和测试集的比例\n",
    "        self.threshold = 0.5 \n",
    "        \n",
    "        ## 保存模型参数\n",
    "        self['checkpoint_dir'] = \"../model/AdversarialLSTM/imdb/checkpoint\"\n",
    "        self['summary_dir'] = \"../model/AdversarialLSTM/imdb/summary\"\n",
    "        self['max_to_keep'] = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重写数据加载类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T08:56:58.998658Z",
     "start_time": "2019-08-14T08:56:58.988985Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataloader(Dataset):\n",
    "    def __init__(self, config):\n",
    "        super(Dataloader, self).__init__(config)\n",
    "        \n",
    "        self.indexFreqs = []    # 统计词空间中每个词出现在多少个不同文档中\n",
    "        \n",
    "    \n",
    "    def _genVocabulary(self, reviews, labels, path, prefix=\"\"):\n",
    "        '''\n",
    "        生成向量和词汇-索引映射字典\n",
    "        '''\n",
    "\n",
    "        save_path = \"../data/wordJson\"\n",
    "        target_word_dir = os.path.join(save_path, prefix + \"_word2idx.json\")\n",
    "        target_label_dir = os.path.join(save_path, prefix + \"_label2idx.json\")\n",
    "\n",
    "  \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        wordCount = Counter(subWords)  # 统计各个词的词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "\n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words, path)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "\n",
    "        # print(len(vocab), vocab[10])\n",
    "        word2idx = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "        ##------------------------------------------------\n",
    "        # 得到逆词频\n",
    "        self._getWordIndexFreq(vocab, reviews, word2idx)\n",
    "        ##------------------------------------------------\n",
    "\n",
    "        uniqueLabel = list(set(labels))\n",
    "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
    "        self.labelList = list(range(len(uniqueLabel)))\n",
    "\n",
    "        # 将词汇表-索引映射表保存为json数据，之后inference时直接加载处理数据\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        with open(target_word_dir, \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(word2idx, f)\n",
    "\n",
    "        with open(target_label_dir, \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(label2idx, f)\n",
    "\n",
    "        return word2idx, label2idx\n",
    "        \n",
    "        \n",
    "    def _getWordIndexFreq(self, vocab, reviews, word2idx):\n",
    "        '''\n",
    "        统计词汇空间中每个词出现在多少个不同的文本中\n",
    "        '''\n",
    "        print(\"正在计算逆词频...\")\n",
    "        indexFreqs = [0] * len(vocab)\n",
    "        for word in tqdm(vocab):\n",
    "            count = 0 \n",
    "            for review in reviews:\n",
    "                if word in set(review):\n",
    "                    count += 1 \n",
    "            indexFreqs[word2idx[word]] = count\n",
    "        \n",
    "        print(\"逆词频计算结束...\")\n",
    "        self.indexFreqs = indexFreqs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T11:34:05.980896Z",
     "start_time": "2019-08-14T11:34:05.949144Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdversarialLSTM(BaseModel):\n",
    "    def __init__(self, config, wordEmbedding, indexFreqs):\n",
    "        super(AdversarialLSTM, self).__init__(config)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        # 表示每个单词的逆词频\n",
    "        ## 第一个词表示PAD，第二个词表示UNK，需要赋默认值\n",
    "        indexFreqs[0], indexFreqs[1] = 20000, 10000 \n",
    "        self.indexFreqs = indexFreqs\n",
    "        ## 根据逆词频计算权重\n",
    "        self.wordWeights = tf.cast(tf.reshape(indexFreqs/tf.reduce_sum(indexFreqs),\n",
    "                                             [1, len(indexFreqs)]),\n",
    "                                  dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        self.build_model()\n",
    "        self.init_saver()\n",
    "        \n",
    "    def build_model(self):\n",
    "        # 定义模型输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, self.config['sequenceLength']],\n",
    "                                    name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutProb = tf.placeholder(tf.float32, name=\"dropoutProb\")\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            ## 利用词频计算新的词嵌入矩阵\n",
    "            normWordEmbedding = self._normalize(tf.cast(self.wordEmbedding, \n",
    "                                                       dtype=tf.float32, \n",
    "                                                       name=\"word2vec\"), self.wordWeights)\n",
    "            ## 利用词嵌入矩阵将输入数据中的词转换为词向量，[batch_size, sequence_length, embed_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(normWordEmbedding, self.inputX)\n",
    "            \n",
    "            \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            with tf.variable_scope(\"Bi-LSTM\", reuse=None):\n",
    "                self.logits = self._Bi_LSTMAttention(self.embeddedWords)\n",
    "                \n",
    "                if self.config['numClasses'] == 1: \n",
    "                    self.predictions = tf.nn.sigmoid(self.logits)\n",
    "                    losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                    labels=tf.cast(\n",
    "                                                                        tf.reshape(self.inputY, [-1, 1]),\n",
    "                                                                        dtype=tf.float32))\n",
    "                elif self.config['numClasses'] > 1: \n",
    "                    self.predictions = tf.nn.softmax(self.logits, dim=1)\n",
    "                    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                           labels=self.inputY)\n",
    "                loss = tf.reduce_mean(losses)\n",
    "                \n",
    "        with tf.name_scope(\"perturLoss\"):\n",
    "            with tf.variable_scope(\"Bi-LSTM\", reuse=True):\n",
    "                perturWordEmbedding = self._addPerturbation(self.embeddedWords, loss)\n",
    "                perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n",
    "                perturLosses = tf.nn.sigmoid_cross_entropy_with_logits(logits=perturPredictions,\n",
    "                                                                      labels=tf.cast(tf.reshape(self.inputY, [-1, 1]),\n",
    "                                                                                    dtype=tf.float32))\n",
    "                perturLoss = tf.reduce_mean(perturLosses)\n",
    "                \n",
    "        self.loss = loss + perturLoss\n",
    "        \n",
    "        # 对所有节点进行更新\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.train_op = tf.train.AdamOptimizer(\n",
    "                    self.config[\"learningRate\"]).minimize(self.loss, \n",
    "                                                          global_step=self.global_step_tensor)\n",
    "        \n",
    "        \n",
    "    def _Bi_LSTMAttention(self, embeddedWords):\n",
    "        '''\n",
    "        Bi-LSTM + Attention结构\n",
    "        '''\n",
    "        \n",
    "        # 定义双向的LSTM\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            ## 定义前向的LSTM结构\n",
    "            lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=self.config['hiddenSizes'],\n",
    "                                                                              state_is_tuple=True),\n",
    "                                                      output_keep_prob=self.dropoutProb)\n",
    "            lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=self.config['hiddenSizes'],\n",
    "                                                                              state_is_tuple=True),\n",
    "                                                      output_keep_prob=self.dropoutProb)\n",
    "            \n",
    "            ## 采用动态RNN，可以动态的输入序列的长度，没有输入则取序列全场\n",
    "            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell,\n",
    "                                                                         lstmBwCell,\n",
    "                                                                         embeddedWords,\n",
    "                                                                         dtype=tf.float32, \n",
    "                                                                         scope=\"bi-lstm\")\n",
    "        \n",
    "        # 将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            ## [batch, max_time, hidden_size]\n",
    "            H = outputs[0] + outputs[1]\n",
    "            ## 得到Attention的输出\n",
    "            output = self._attention(H)\n",
    "           \n",
    "        # 全连接层输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            pred = tf.layers.dense(output, self.config['numClasses'],name=\"dense\",\n",
    "                                         kernel_initializer=tf.truncated_normal_initializer(stddev=0.1, seed=2019),\n",
    "                                         bias_initializer=tf.constant_initializer(0.1))\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def _attention(self, H):\n",
    "        '''\n",
    "        利用Attention机制得到句子的向量表示\n",
    "        '''\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = self.config[\"hiddenSizes\"]\n",
    "        \n",
    "        # 初始化一个查询向量query\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        # 对Bi-LSTM的输出用激活函数做非线性变换\n",
    "        M = tf.tanh(H) \n",
    "        \n",
    "        # 对M和W做矩阵运算，得到每一个时间步的权重，newM的大小 [batch_size, time_step, 1]\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # 定义newM做维度转换 [batch_size, time_step]\n",
    "        restoreM = tf.reshape(newM, [-1, self.config['sequenceLength']])\n",
    "     \n",
    "        # 对权重进行归一化处理\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的alpha的值对H进行加权求和\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, \n",
    "                                                             [-1, self.config[\"sequenceLength\"], 1]))\n",
    "        # 将三维压缩成二维 [batch_size, hidden_size]\n",
    "        sequeezeR = tf.squeeze(r, axis=2)\n",
    "        sentenceRepren = tf.tanh(sequeezeR)\n",
    "        \n",
    "        # 对Attention的输出可以做dropout处理\n",
    "        output = tf.nn.dropout(sentenceRepren, self.dropoutProb)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _normalize(self, wordEmbedding, weights):\n",
    "        '''\n",
    "        对word embedding结合权重做标准化处理\n",
    "        '''\n",
    "        # 对所有词向量求加权均值\n",
    "        mean = tf.matmul(weights, wordEmbedding)\n",
    "        powWordEmbedding = tf.pow(wordEmbedding - mean, 2)\n",
    "        \n",
    "        var = tf.matmul(weights, powWordEmbedding)\n",
    "        stddev = tf.sqrt(1e-6+var)\n",
    "        \n",
    "        return (wordEmbedding - mean) / stddev\n",
    "    \n",
    "    def _addPerturbation(self, embeddedWords, loss):\n",
    "        '''\n",
    "        对此向量添加波动\n",
    "        embeddedWords: 这里表示加权后的词向量, [batch, max_time, embed_size]\n",
    "        '''\n",
    "        grad, = tf.gradients(loss, embeddedWords, \n",
    "                            aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n",
    "        ## 返回值和原值相等，只不过截断向前的梯度\n",
    "        grad = tf.stop_gradient(grad)\n",
    "        perturb = self._scaleL2(grad, self.config['epsilon'])\n",
    "        return embeddedWords + perturb\n",
    "    \n",
    "    def _scaleL2(self, x, norm_length):\n",
    "        '''\n",
    "        x中每个batch的元素都除以这个batch中经过l2 not稳定之后的最大值\n",
    "        l2norm(x) = a * l2norm(x/a)\n",
    "        x: 大小为[batch, max_time, embed_size]\n",
    "        '''\n",
    "        alpha = tf.reduce_max(tf.abs(x), (1, 2), keepdims=True) + 1e-12\n",
    "        l2_norm = alpha * tf.sqrt(tf.reduce_sum(tf.pow(x/alpha, 2), (1, 2), \n",
    "                                               keepdims=True)+1e-6)\n",
    "        x_unit = x / l2_norm\n",
    "        return norm_length * x_unit\n",
    "    \n",
    "    def init_saver(self):\n",
    "        '''\n",
    "        初始化用于保存模型的对象\n",
    "        '''\n",
    "        self.saver = tf.train.Saver(max_to_keep=self.config['max_to_keep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T11:08:44.087364Z",
     "start_time": "2019-08-14T11:08:44.072693Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer(BaseTrain):\n",
    "    def __init__(self, sess, model, data, config, logger):\n",
    "        super(Trainer, self).__init__(sess, model, data, config, logger)\n",
    "        self.train = data[0]\n",
    "        self.eval = data[1]\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        num_iter_per_epoch = self.train.length // self.config['batch_size']\n",
    "        \n",
    "        for _ in tqdm(range(num_iter_per_epoch)):\n",
    "            ## 获取训练过程的结果\n",
    "            loss, metrics, step = self.train_step()\n",
    "            train_acc = metrics[\"accuracy\"]\n",
    "            train_f_score = metrics[\"f_score\"]\n",
    "            \n",
    "            ## 将训练过程中的损失写入\n",
    "            summaries_dict = {\"loss\": loss, \n",
    "                             \"acc\": np.array(train_acc), \n",
    "                             \"f_score\": np.array(train_f_score)}\n",
    "            self.logger.summarize(step, summarizer=\"train\", scope=\"train_summary\",\n",
    "                                 summaries_dict=summaries_dict)\n",
    "            \n",
    "            if step % self.config['evaluateEvery'] == 0:\n",
    "                print(\"Train —— Step: {} | Loss: {} | Acc: {} | F1_Score: {}\".format(\n",
    "                    step, loss, train_acc, train_f_score))\n",
    "                ## 对测试集进行评估\n",
    "                print(\"\\nEvaluation: \\n\")\n",
    "                eval_losses = []\n",
    "                eval_true = []\n",
    "                eval_pred = []\n",
    "                \n",
    "                for batchEval in self.eval.iter_all(self.config[\"batch_size\"]):\n",
    "                    loss, predictions = self.eval_step(batchEval[0], batchEval[1])\n",
    "                    eval_losses.append(loss)\n",
    "                    eval_true.extend(batchEval[-1])\n",
    "                    eval_pred.extend(predictions)\n",
    "                \n",
    "                getMetric = Metric(np.array(eval_pred), np.array(eval_true), self.config)\n",
    "                metrics = getMetric.get_metrics()\n",
    "                loss_mean = np.round(np.mean(eval_losses), 5)\n",
    "                prec_mean = np.round(metrics[\"precision\"])\n",
    "                recall_mean = np.round(metrics[\"recall\"])\n",
    "                time_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M%S %p\")\n",
    "                \n",
    "                print(\"{} | Loss: {} | Precision: {} | Recall: {}\".format(time_str,\n",
    "                                                                         loss_mean,\n",
    "                                                                         prec_mean,\n",
    "                                                                         recall_mean))\n",
    "                \n",
    "                summaries_dict = {\"loss\": np.array(loss_mean),\n",
    "                                 \"precision\": np.array(prec_mean),\n",
    "                                 \"recall\": np.array(recall_mean)}\n",
    "                self.logger.summarize(step, summarizer=\"test\", scope=\"test_summary\",\n",
    "                                     summaries_dict=summaries_dict)\n",
    "                \n",
    "            if step % self.config[\"checkpointEvery\"] == 0: \n",
    "                self.model.save(self.sess)\n",
    "        \n",
    "    def train_step(self):\n",
    "        batch_x, batch_y = next(self.train.next_batch(self.config['batch_size']))\n",
    "        feed_dict = {self.model.inputX: batch_x, \n",
    "                    self.model.inputY: batch_y, \n",
    "                    self.model.dropoutProb: self.config['dropoutProb']}\n",
    "        \n",
    "        _, loss, predictions, step = self.sess.run([self.model.train_op,\n",
    "                                                   self.model.loss, \n",
    "                                                   self.model.predictions,\n",
    "                                                   self.model.global_step_tensor],\n",
    "                                                  feed_dict=feed_dict)\n",
    "        getMetric = Metric(predictions, batch_y, self.config)\n",
    "        metrics = getMetric.get_metrics()\n",
    "            \n",
    "        return loss, metrics, step\n",
    "    \n",
    "    def eval_step(self, batch_x, batch_y):\n",
    "        '''\n",
    "        使用验证集数据进行测试\n",
    "        '''\n",
    "        feed_dict = {self.model.inputX: batch_x,\n",
    "                    self.model.inputY: batch_y,\n",
    "                    self.model.dropoutProb: 1.0}\n",
    "        loss, predictions = self.sess.run([self.model.loss,  self.model.predictions],\n",
    "                                         feed_dict=feed_dict)\n",
    "        \n",
    "        return loss, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用数据集进行训练和预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用IMDB数据集进行训练和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T09:50:11.274561Z",
     "start_time": "2019-08-14T08:57:24.756184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在计算逆词频...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fb00abfa05463685285d28c55f3d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=26679), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "逆词频计算结束...\n"
     ]
    }
   ],
   "source": [
    "# 实例化配置参数，指定训练数据的文件名\n",
    "path = \"../data/imdb/labeldTrain.csv\"\n",
    "config = Config(path)\n",
    "create_dirs([config['summary_dir'], config['checkpoint_dir']])\n",
    "\n",
    "data = Dataloader(config)\n",
    "# 生成训练数据，第一个参数表示wordEmbedding文件所在的文件夹\n",
    "data.dataGen(\"../data/imdb/\", prefix=\"imdb\")\n",
    "\n",
    "train_X, train_y, eval_X, eval_y = data.trainReviews, data.trainLabels, data.evalReviews, data.evalLabels\n",
    "wordEmbedding, labels = data.wordEmbedding, data.labelList\n",
    "indexFreqs = data.indexFreqs\n",
    "\n",
    "train_data = DataGenerator(train_X, train_y)\n",
    "eval_data = DataGenerator(eval_X, eval_y)\n",
    "pack_data = [train_data, eval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T11:34:11.190017Z",
     "start_time": "2019-08-14T11:34:11.184554Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    tf.reset_default_graph()\n",
    "    # 设置计算图配置\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9 \n",
    "    \n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    ## 创建一个实例\n",
    "    model = AdversarialLSTM(config, wordEmbedding, indexFreqs)\n",
    "    \n",
    "    logger = Logger(sess, config)\n",
    "    \n",
    "    trainer = Trainer(sess, model, pack_data, config, logger)\n",
    "    trainer.train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T11:46:35.474085Z",
     "start_time": "2019-08-14T11:34:11.370454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前正处于第1次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9f0bb5ec0b46c987a1f430ce283439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 100 | Loss: 0.8709119558334351 | Acc: 0.875 | F1_Score: 0.86207\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:3506 PM | Loss: 0.9097700119018555 | Precision: 0.8886 | Recall: 0.8294\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 200 | Loss: 0.4036068916320801 | Acc: 0.95312 | F1_Score: 0.95774\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:3555 PM | Loss: 0.5567100048065186 | Precision: 0.86655 | Recall: 0.88886\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 300 | Loss: 0.5169576406478882 | Acc: 0.84375 | F1_Score: 0.83871\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:3643 PM | Loss: 0.41095998883247375 | Precision: 0.88746 | Recall: 0.86877\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第2次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f4d48d166f4aaf80e2ad55f7d8ef00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 400 | Loss: 0.24110829830169678 | Acc: 0.90625 | F1_Score: 0.9\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:3731 PM | Loss: 0.37959998846054077 | Precision: 0.87972 | Recall: 0.88928\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 500 | Loss: 0.346789687871933 | Acc: 0.90625 | F1_Score: 0.91176\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:3819 PM | Loss: 0.40165001153945923 | Precision: 0.92577 | Recall: 0.80821\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 600 | Loss: 0.1802903711795807 | Acc: 0.96875 | F1_Score: 0.97222\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:3906 PM | Loss: 0.3541400134563446 | Precision: 0.89149 | Recall: 0.87223\n",
      "Saving model...\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第3次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64835a42d5a41b3827751c5f065c977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 700 | Loss: 0.14265277981758118 | Acc: 0.95312 | F1_Score: 0.95238\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:3954 PM | Loss: 0.35947999358177185 | Precision: 0.8829 | Recall: 0.89023\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 800 | Loss: 0.3021504878997803 | Acc: 0.92188 | F1_Score: 0.92754\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:4042 PM | Loss: 0.36469998955726624 | Precision: 0.88327 | Recall: 0.89741\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 900 | Loss: 0.18523581326007843 | Acc: 0.9375 | F1_Score: 0.9375\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:4129 PM | Loss: 0.3753899931907654 | Precision: 0.89133 | Recall: 0.87786\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第4次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8c49d5a7ac4f999e6c326a6478bd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 1000 | Loss: 0.3152977228164673 | Acc: 0.89062 | F1_Score: 0.87273\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:4217 PM | Loss: 0.4073899984359741 | Precision: 0.8881 | Recall: 0.88833\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1100 | Loss: 0.24868708848953247 | Acc: 0.9375 | F1_Score: 0.92\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:4304 PM | Loss: 0.4018099904060364 | Precision: 0.90764 | Recall: 0.83727\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1200 | Loss: 0.2673550248146057 | Acc: 0.90625 | F1_Score: 0.90625\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:4352 PM | Loss: 0.42267999053001404 | Precision: 0.90059 | Recall: 0.8507\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第5次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cff011eacf4de3ad5f9fc8a58df89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —— Step: 1300 | Loss: 0.2000146508216858 | Acc: 0.96875 | F1_Score: 0.96774\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:4440 PM | Loss: 0.4203700125217438 | Precision: 0.87252 | Recall: 0.89767\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1400 | Loss: 0.24702349305152893 | Acc: 0.95312 | F1_Score: 0.95082\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:4527 PM | Loss: 0.4314900040626526 | Precision: 0.88518 | Recall: 0.86626\n",
      "Saving model...\n",
      "Model saved\n",
      "Train —— Step: 1500 | Loss: 0.18581444025039673 | Acc: 0.95312 | F1_Score: 0.94915\n",
      "\n",
      "Evaluation: \n",
      "\n",
      "2019-08-14 19:4615 PM | Loss: 0.445609986782074 | Precision: 0.88772 | Recall: 0.87582\n",
      "Saving model...\n",
      "Model saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
