{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T08:57:45.327650Z",
     "start_time": "2019-08-30T08:57:44.634137Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen/anaconda3/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub \n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "from tqdm.autonotebook import tqdm \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T08:57:47.009864Z",
     "start_time": "2019-08-30T08:57:46.790949Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T08:57:47.984491Z",
     "start_time": "2019-08-30T08:57:47.640134Z"
    }
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import spacy\n",
    "import nltk \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据清洗\n",
    "def clean_text(text):\n",
    "    # 移除url\n",
    "    text = re.sub(r\"http\\S+\",\"\", text)\n",
    "    # 移除标点符号\n",
    "    punctuation = \"''#$%^&*+-<>=:/@{}|~\\\\/;()`,.\"\n",
    "    #text = \"\".join([ch for ch in text if ch not in set(punctuation)])\n",
    "    text = text.lower()\n",
    "    text = text.replace(r\"[0-9]\", \"\")\n",
    "    # 对单词进行标准化\n",
    "    #nlp = spacy.load(\"en\", diable=[\"parser\", \"ner\"])\n",
    "    #text = \" \".join([token.lemma_ for token in nlp(text)])\n",
    "    \n",
    "    stoplist = stopwords.words(\"english\")\n",
    "    text = \" \".join([token for token in text.split() if token.strip() not in stoplist and token.strip() not in set(punctuation)])\n",
    "    # 移除空格\n",
    "    text = \" \".join(text.split())\n",
    "    # 超过200的只取前200个\n",
    "    text = \" \".join(text.split()[:200])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置基础的配置\n",
    "class Config(dict):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #定义训练参数\n",
    "        self['num_epochs'] = 1 \n",
    "        self['batch_size'] = 32 \n",
    "        self['sequenceLength'] = 200\n",
    "        self['evaluateEvery'] = 200 \n",
    "        self['checkpointEvery'] = 200 \n",
    "        \n",
    "        # 学习率衰减\n",
    "        self['learningRate'] = 0.01 \n",
    "        self['decay_steps'] = 100\n",
    "        self['decay_rate'] = 0.9 \n",
    "        self['grad_clip'] = 4.0 \n",
    "        \n",
    "        # 定义模型参数\n",
    "        self['embeddingSize'] = 1024\n",
    "        self['dropoutProb'] = 0.5\n",
    "        self['l2RegLambda'] = 0.001\n",
    "        self['hiddenSizes'] = [128]\n",
    "        \n",
    "        # 设置基础参数\n",
    "        self['numClasses'] = 1\n",
    "        self['train_size'] = 0.8 \n",
    "        self.threshold = 0.5 \n",
    "        \n",
    "        # 保存模型的参数\n",
    "        self['checkpoint_dir'] = \"../model/ELMO/imdb/checkpoint\"\n",
    "        self['summary_dir'] = \"../model/ELMO/imdb/summary\"\n",
    "        self['max_to_keep'] = 5\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 定义模型类和训练类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 定义模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMAttention(BaseModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "        self.build_model()\n",
    "        self.init_saver()\n",
    "        \n",
    "    def build_model(self):\n",
    "        # 输入层\n",
    "        self.inputX = tf.placeholder(tf.string, [None], name=\"inputX\")\n",
    "        #self.inputL = tf.placeholder(tf.int32, [None], name=\"inputL\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None], name=\"inputY\")\n",
    "        \n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        # 输出 [batch, seq_len, 1024]\n",
    "        elmo_result = self.elmo(self.inputX, signature='default', as_dict=True)\n",
    "        self.embededWords = elmo_result[\"elmo\"]\n",
    "        \n",
    "        ## 如果序列的最大长度小于200，则剩余部分用全0向量填充\n",
    "        ## 如果长度不足则填充\n",
    "        #if current_len < self.config['sequenceLength']:\n",
    "        #self.embededWords = tf.pad(self.embededWords, \n",
    "        #                               paddings=[[0,0],[0, self.config[\"sequenceLength\"]-current_len], [0, 0]])\n",
    "            \n",
    "        \n",
    "        # 定义双向LSTM模型\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            for idx, hiddenSize in enumerate(self.config['hiddenSizes']):\n",
    "                with tf.name_scope(f\"Layer_{idx}\"):\n",
    "                    ## 定义前向的LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize,\n",
    "                                                                                  state_is_tuple=True),\n",
    "                                                              output_keep_prob=self.dropout_keep_prob)\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize,\n",
    "                                                                                  state_is_tuple=True),\n",
    "                                                              output_keep_prob=self.dropout_keep_prob)\n",
    "                    \n",
    "                    # 采用动态RNN\n",
    "                    outputs_, current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell,\n",
    "                                                                             self.embededWords,\n",
    "                                                                             dtype=tf.float32,\n",
    "                                                                             scope=f\"bi-lstm_{idx}\")\n",
    "                    # 对outputs的fw和bw结果进行拼接\n",
    "                    ## [batch_size, seq_len, hidden_size*2]\n",
    "                    self.embededWords = tf.concat(outputs_, 2)\n",
    "                    \n",
    "        ## 按照最后一个维度进行切分\n",
    "        outputs  = tf.split(self.embededWords, 2, -1)\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            ## 维度是 [batch, seq_len, hidden_size]\n",
    "            H = outputs[0] + outputs[1]\n",
    "            ## 得到Attention的输出\n",
    "            output = self._attention(H)\n",
    "            outputSize = self.config['hiddenSizes'][-1]\n",
    "            \n",
    "        # 输出层\n",
    "        with tf.name_scope(\"output\"):\n",
    "            self.logits = tf.layers.dense(output, self.config['numClasses'],\n",
    "                                         kernel_initializer=tf.initializers.glorot_normal(),\n",
    "                                         bias_initializer=tf.initializers.constant(0.1))\n",
    "            self.predictions = tf.nn.sigmoid(self.logits)\n",
    "            \n",
    "        # 损失的计算\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(tf.reshape(self.inputY, [-1, 1]),\n",
    "                                                                           dtype=tf.float32),\n",
    "                                                            logits=self.logits)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            if self.config['l2RegLambda']:\n",
    "                with tf.variable_scope(\"dense\", reuse=True):\n",
    "                    outputW = tf.get_variable(\"kernel\")\n",
    "                    l2_loss += tf.nn.l2_loss(outputW)\n",
    "                    \n",
    "            self.loss += self.config['l2RegLambda'] * l2_loss\n",
    "        \n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            learning_rate = tf.train.exponential_decay(self.config['learningRate'],\n",
    "                                                      self.global_step_tensor,\n",
    "                                                      self.config['decay_steps'],\n",
    "                                                      self.config['decay_rate'],\n",
    "                                                      staircase=True)\n",
    "            ## 使用梯度削减防止梯度爆炸\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "            for idx, (grad, var) in enumerate(grads_and_vars):\n",
    "                if grad is not None:\n",
    "                    grads_and_vars[idx] = (tf.clip_by_norm(grad, self.config['grad_clip']), var)\n",
    "                    \n",
    "            self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step_tensor)\n",
    "            \n",
    "    # 定义注意力结构\n",
    "    def _attention(self, H):\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = self.config['hiddenSizes'][-1]\n",
    "        # 初始化key\n",
    "        key = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1), name=\"key\")\n",
    "        # 对Bi-LSTM的结果进行激活\n",
    "        M = tf.tanh(H)\n",
    "        ## 形状 [batch*seq_len, 1]\n",
    "        restoreM = tf.tensordot(M, key, axes=((2), (0)))\n",
    "        #restoreM = tf.squeeze(newM, 2)\n",
    "        # 用归一化除以得到 [batch, seq_len]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的权重对H进行加权求和\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.expand_dims(self.alpha, 2))\n",
    "        output = tf.tanh(tf.squeeze(r, 2))\n",
    "        # 进行dropout处理\n",
    "        output = tf.nn.dropout(output, self.dropout_keep_prob)\n",
    "        return output\n",
    "    \n",
    "    def init_saver(self):\n",
    "        self.saver = tf.train.Saver(max_to_keep=self.config['max_to_keep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(BaseTrain):\n",
    "    def __init__(self, sess, model, data, config, logger):\n",
    "        super().__init__(sess, model, data, config, logger)\n",
    "        self.train = data[0]\n",
    "        self.eval = data[1]\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        num_iter_per_epoch = self.train.length // self.config['batch_size']\n",
    "        for i in tqdm(range(num_iter_per_epoch)):\n",
    "            ## 获取训练结果\n",
    "            loss, metrics, step = self.train_step()\n",
    "            train_acc = metrics['accuracy']\n",
    "            train_f_score = metrics['f_score']\n",
    "            \n",
    "            # 将训练过程的损失写入\n",
    "            summaries_dict = {\"loss\": loss, \n",
    "                             \"acc\": np.array(train_acc),\n",
    "                             \"f_score\": np.array(train_f_score)}\n",
    "            self.logger.summarize(step, summarizer=\"train\", scope=\"train_summary\",\n",
    "                                 summaries_dict=summaries_dict)\n",
    "            if step % self.config['evaluateEvery'] == 0: \n",
    "                print(\"Train —— Step: {} | Loss: {} | Acc: {} : F1_Score: {}\".format(\n",
    "                    step, loss, train_acc, train_f_score))\n",
    "                # 对测试集进行评估\n",
    "                eval_losses = []\n",
    "                eval_pred = []\n",
    "                eval_true = []\n",
    "                for batchEval in self.eval.iter_all(self.config['batch_size']):\n",
    "                    loss, predictions = self.eval_step(batchEval)\n",
    "                    eval_losses.append(loss)\n",
    "                    eval_pred.extend(predictions)\n",
    "                    eval_true.extend(batchEval[-1])\n",
    "                getMetric = Metric(np.array(eval_pred), np.array(eval_true),\n",
    "                                  self.config)\n",
    "                metrics = getMetric.get_metrics()\n",
    "                eval_prec = np.round(metrics['precision'], 5)\n",
    "                eval_recall = np.round(metrics['recall'], 5)\n",
    "                loss_mean = np.round(np.mean(eval_losses), 5)\n",
    "                print(\"Evaluation —— Loss: {} | Precision: {} | Recall: {}\".format(\n",
    "                    loss_mean, eval_prec, eval_recall))\n",
    "                summaries_dict = {\"loss\": np.array(loss_mean),\n",
    "                                 \"precision\": np.array(eval_prec), \n",
    "                                 \"recall\": np.array(eval_recall)}\n",
    "                self.logger.summarize(step, summarizer=\"test\", scope=\"test_summary\",\n",
    "                                     summaries_dict=summaries_dict)\n",
    "            if step % self.config['checkpointEvery'] == 0: \n",
    "                self.model.save(self.sess)\n",
    "            \n",
    "            \n",
    "    def train_step(self):\n",
    "        batch_x,  batch_y = next(self.train.next_batch(self.config['batch_size']))\n",
    "        feed_dict = {self.model.inputX: batch_x,\n",
    "                     #self.model.inputL: batch_len, \n",
    "                    self.model.inputY: batch_y,\n",
    "                    self.model.dropout_keep_prob: self.config['dropoutProb']}\n",
    "    \n",
    "        _, loss, predictions, step = self.sess.run([self.model.train_op,\n",
    "                                                   self.model.loss,\n",
    "                                                   self.model.predictions, \n",
    "                                                   self.model.global_step_tensor],\n",
    "                                                  feed_dict=feed_dict)\n",
    "        getMetric = Metric(predictions, batch_y, self.config)\n",
    "        metrics = getMetric.get_metrics()\n",
    "        return loss, metrics, step\n",
    "    \n",
    "    def eval_step(self, *batch):\n",
    "        feed_dict = {self.model.inputX: batch[0],\n",
    "                     #self.model.inputL: batch[1],\n",
    "                    self.model.inputY: batch[-1],\n",
    "                    self.model.dropout_keep_prob: 1.0}\n",
    "        loss, predictions = self.sess.run([self.model.loss, self.model.predictions],\n",
    "                                         feed_dict=feed_dict)\n",
    "        return loss, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv(\"../data/imdb/labeldTrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb['review'] = imdb['review'].apply(clean_text)\n",
    "\n",
    "#imdb['length'] = imdb['review'].apply(lambda s: len(s.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_inputs(reviews, length, config):\n",
    "    outputs = []\n",
    "    for i in range(len(reviews)):\n",
    "        review = reviews[i].split() + [\"\"] * (config['sequenceLength'] - length[i])\n",
    "        outputs.append(review)\n",
    "    return np.array(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, y, *x):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.length = len(y)\n",
    "        ## 计算不同类别的比例\n",
    "        unique = Counter(self.y.ravel())\n",
    "        self.ratio = [(key, value / self.length) for key, value in unique.items()]\n",
    "        self.indices = []\n",
    "        for key, _ in self.ratio:\n",
    "            index = np.where(y.ravel() == key)\n",
    "            self.indices.append(index)\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        '''\n",
    "        生成每一个batch的数据集\n",
    "        '''\n",
    "        choose = np.array([])\n",
    "        for i in range(len(self.indices)):\n",
    "            idx = np.random.choice(self.indices[i][0],\n",
    "                                   max(1, min(len(self.indices[i][0]), int(batch_size * self.ratio[i][1]))))\n",
    "            choose = np.append(choose, idx)\n",
    "        choose = np.random.permutation(choose).astype(\"int64\")\n",
    "\n",
    "        result = []\n",
    "        for item in self.x:\n",
    "            result.append(item[choose])\n",
    "        result.append(self.y[choose])\n",
    "        yield result\n",
    "\n",
    "    def iter_all(self, batch_size):\n",
    "        '''\n",
    "        按照batch迭代所有数据\n",
    "        '''\n",
    "        numBatches = self.length // batch_size + 1\n",
    "        for i in range(numBatches):\n",
    "            result = []\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, self.length)\n",
    "            for item in self.x:\n",
    "                result.append(np.array(item[start:end]))\n",
    "            batchY = np.array(self.y[start:end], dtype=\"float32\")\n",
    "            result.append(batchY)\n",
    "            yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    config = Config()\n",
    "    reviews = imdb['review'].values\n",
    "    labels = imdb['sentiment'].values\n",
    "    #length = imdb['length'].values\n",
    "    \n",
    "    #reviews = pad_inputs(reviews, length, config)\n",
    "    ## 表示总的数量\n",
    "    nums = imdb.shape[0]\n",
    "    \n",
    "    \n",
    "    train_idx = slice(0, int(nums * config['train_size']))\n",
    "    val_idx = slice(int(nums*config['train_size']), nums)\n",
    "    \n",
    "    train_X,  train_y, eval_X,  eval_y = reviews[train_idx],   labels[train_idx], reviews[val_idx],  labels[val_idx]\n",
    "    train_data = DataGenerator(train_y, train_X)\n",
    "    eval_data = DataGenerator(eval_y, eval_X)\n",
    "    pack_data = [train_data, eval_data]\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    ## 设置计算图的配置\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9 \n",
    "    \n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = BiLSTMAttention(config)\n",
    "    logger = Logger(sess, config)\n",
    "    \n",
    "    trainer = Trainer(sess, model, pack_data, config, logger)\n",
    "    trainer.train_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
