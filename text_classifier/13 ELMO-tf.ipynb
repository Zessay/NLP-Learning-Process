{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen/anaconda3/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from bilm import TokenBatcher, BidirectionalLanguageModel, weight_layers, dump_token_embeddings, Batcher\n",
    "\n",
    "from tqdm.autonotebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 配置和加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, path):\n",
    "        # 训练参数\n",
    "        self['num_epochs'] = 10 \n",
    "        self['learningRate'] = 0.01 \n",
    "        self['decay_steps'] = 200\n",
    "        self['decay_rate'] = 0.9 \n",
    "        self['grad_clip'] = 5.0 \n",
    "        self['evaluateEvery'] = 200 \n",
    "        self['checkpointEvery'] = 200 \n",
    "\n",
    "        # 模型参数\n",
    "        self['embeddingSize'] = 256   # 也就是elmo最终输出的参数\n",
    "        self['hiddenSizes'] = [128]     # LSTM神经元的个数\n",
    "        self['dropoutProb'] = 0.5 \n",
    "        self['l2RegLambda'] = 0.00 \n",
    "        self['sequenceLength'] = 200\n",
    "        self['batch_size'] = 64 \n",
    "\n",
    "        # 基础参数\n",
    "        self['dataSource'] = path\n",
    "        self['stopWordSource'] = \"../data/english\"\n",
    "        self['optionFile'] = \"elmo_parameters/elmo_2x1024_128_2048cnn_1xhighway_options.json\"\n",
    "        self['weightFile'] = \"elmo_parameters/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5\"\n",
    "        self['vocabFile'] = \"elmo_parameters/vocab.txt\"\n",
    "        self['tokenEmbeddingFile'] = \"elmo_parameters/elmo_token_embeddings.hdf5\"\n",
    "        self['numClasses'] = 1\n",
    "        self['train_size'] = 0.8   # 训练集数据占比\n",
    "        self.threshold = 0.5 \n",
    "        \n",
    "        # 保存模型参数\n",
    "        self['checkpoint_dir'] = \"../model/ELMO/imdb/checkpoint\"\n",
    "        self['summary_dir'] = \"../model/ELMO/imdb/summary\"\n",
    "        self['max_to_keep'] = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config['dataSource']\n",
    "        self._stopWordSource = config['stopWordSource']\n",
    "        self._optionFile = config['optionFile']\n",
    "        self._weightFile = config['weightFile']\n",
    "        self._vocabFile = config['vocabFile']\n",
    "        self._tokenEmbeddingFile = config['tokenEmbeddingFile']\n",
    "        \n",
    "        self._sequenceLength = config['sequenceLength']  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config['embeddingSize']\n",
    "        self._bathSize = config['batch_size']\n",
    "        self._rate = config['train_size']\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        '''\n",
    "        从csv文件中读取数据集\n",
    "        '''\n",
    "        print(1)\n",
    "        df = pd.read_csv(filePath)\n",
    "        review = df[\"review\"].tolist()\n",
    "        labels = None\n",
    "        if \"sentiment\" in df.columns:\n",
    "            labels = df['sentiment'].tolist()\n",
    "        elif \"emotion\" in df.columns:\n",
    "            labels = df['emotion'].tolist()\n",
    "        \n",
    "        reviews = [line.strip().split() for line in review]\n",
    "        return reviews, labels\n",
    "    \n",
    "    def _genVocabFile(self, reviews):\n",
    "        '''\n",
    "        用训练数据生成一个词汇文件，并加入3个特殊字符\n",
    "        '''\n",
    "        print(2)\n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        ## 统计词频\n",
    "        wordCount = Counter(allWords)\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 3]\n",
    "        allTokens = [\"<S>\", \"</S>\", \"<UNK>\"] + words\n",
    "        with open(self._vocabFile, \"w\") as fout:\n",
    "            fout.write(\"\\n\".join(allTokens))\n",
    "    \n",
    "    def _fixedSeq(self, reviews):\n",
    "        '''\n",
    "        将长度超过200的句子进行截断\n",
    "        '''\n",
    "        print(3)\n",
    "        return [review[:self._sequenceLength] for review in reviews]\n",
    "    \n",
    "    def _genElmoEmbedding(self):\n",
    "        '''\n",
    "        调用ELMO源码中的dump_token_embeddings方法，基于字符的表示生成词向量表示\n",
    "        并保存为hdf5文件，文件中embedding键对应的value就是词汇表文件中各词汇的向量表示\n",
    "        这些词汇的向量表示之后会作为BiLM的初始化输入\n",
    "        '''\n",
    "        dump_token_embeddings(self._vocabFile, self._optionFile, self._weightFile,\n",
    "                             self._tokenEmbeddingFile)\n",
    "        \n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        '''\n",
    "        生成训练集和验证集\n",
    "        '''\n",
    "        print(4)\n",
    "        y = [[item] for item in y]\n",
    "        trainIndex = int(len(x) * rate)\n",
    "        trainReviews = x[:trainIndex]\n",
    "        trainLabels = y[:trainIndex]\n",
    "        evalReviews = x[trainIndex:]\n",
    "        evalLabels = y[trainIndex:]\n",
    "        \n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "    \n",
    "    def dataGen(self):\n",
    "        '''\n",
    "        初始化训练集和验证集\n",
    "        '''\n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        ## 生成词汇表文件\n",
    "        self._genVocabFile(reviews)\n",
    "        self._genElmoEmbedding()  ## 生成embedding文件\n",
    "        \n",
    "        reviews = self._fixedSeq(reviews)\n",
    "        ## 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMAttention(BaseModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self._init_elmo()\n",
    "        self.build_model()\n",
    "        self.init_saver()\n",
    "        \n",
    "    def build_model(self):\n",
    "        # 输入层\n",
    "        self.inputX = tf.placeholder(tf.float32, [None, self.config['sequenceLength'], self.config['embeddingSize']], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None], name=\"inputY\")\n",
    "        \n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "\n",
    "        # 定义embedding层\n",
    "        with tf.name_scope(\"Embedding\"):\n",
    "            embeddingW = tf.get_variable(\"embeddingW\", \n",
    "                                        shape=[self.config['embeddingSize'], self.config['embeddingSize']],\n",
    "                                        initializer=tf.initializers.glorot_normal())\n",
    "            reshapeInputX = tf.reshape(self.inputX, shape=[-1, self.config['embeddingSize']])\n",
    "            self.embededWords = tf.reshape(tf.matmul(reshapeInputX, embeddingW), \n",
    "                                           shape=[-1, self.config['sequenceLength'], self.config['embeddingSize']])\n",
    "        \n",
    "            self.embededWords = tf.nn.dropout(self.embededWords, self.dropout_keep_prob)\n",
    "        \n",
    "        # 定义双向LSTM模型\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            for idx, hiddenSize in enumerate(self.config['hiddenSizes']):\n",
    "                with tf.name_scope(f\"Layer_{idx}\"):\n",
    "                    ## 定义前向的LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize,\n",
    "                                                                                  state_is_tuple=True),\n",
    "                                                              output_keep_prob=self.dropout_keep_prob)\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize,\n",
    "                                                                                  state_is_tuple=True),\n",
    "                                                              output_keep_prob=self.dropout_keep_prob)\n",
    "                    \n",
    "                    # 采用动态RNN\n",
    "                    outputs_, current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell,\n",
    "                                                                             self.embededWords,\n",
    "                                                                             dtype=tf.float32,\n",
    "                                                                             scope=f\"bi-lstm_{idx}\")\n",
    "                    # 对outputs的fw和bw结果进行拼接\n",
    "                    ## [batch_size, seq_len, hidden_size*2]\n",
    "                    self.embededWords = tf.concat(outputs_, 2)\n",
    "                    \n",
    "        ## 按照最后一个维度进行切分\n",
    "        outputs  = tf.split(self.embededWords, 2, -1)\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            ## 维度是 [batch, seq_len, hidden_size]\n",
    "            H = outputs[0] + outputs[1]\n",
    "            ## 得到Attention的输出\n",
    "            output = self._attention(H)\n",
    "            outputSize = self.config['hiddenSizes'][-1]\n",
    "            \n",
    "        # 输出层\n",
    "        with tf.name_scope(\"output\"):\n",
    "            self.logits = tf.layers.dense(output, self.config['numClasses'],\n",
    "                                         kernel_initializer=tf.initializers.glorot_normal(),\n",
    "                                         bias_initializer=tf.initializers.constant(0.1))\n",
    "            self.predictions = tf.nn.sigmoid(self.logits)\n",
    "            \n",
    "        # 损失的计算\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(tf.reshape(self.inputY, [-1, 1]),\n",
    "                                                                           dtype=tf.float32),\n",
    "                                                            logits=self.logits)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            if self.config['l2RegLambda']:\n",
    "                with tf.variable_scope(\"dense\", reuse=True):\n",
    "                    outputW = tf.get_variable(\"kernel\")\n",
    "                    l2_loss += tf.nn.l2_loss(outputW)\n",
    "                    \n",
    "            self.loss += self.config['l2RegLambda'] * l2_loss\n",
    "        \n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            learning_rate = tf.train.exponential_decay(self.config['learningRate'],\n",
    "                                                      self.global_step_tensor,\n",
    "                                                      self.config['decay_steps'],\n",
    "                                                      self.config['decay_rate'],\n",
    "                                                      staircase=True)\n",
    "            ## 使用梯度削减防止梯度爆炸\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "            for idx, (grad, var) in enumerate(grads_and_vars):\n",
    "                if grad is not None:\n",
    "                    grads_and_vars[idx] = (tf.clip_by_norm(grad, self.config['grad_clip']), var)\n",
    "                    \n",
    "            self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step_tensor)\n",
    "            \n",
    "    # 定义注意力结构\n",
    "    def _attention(self, H):\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = self.config['hiddenSizes'][-1]\n",
    "        # 初始化key\n",
    "        key = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1), name=\"key\")\n",
    "        # 对Bi-LSTM的结果进行激活\n",
    "        M = tf.tanh(H)\n",
    "        ## 形状 [batch*seq_len, 1]\n",
    "        restoreM = tf.tensordot(M, key, axes=((2), (0)))\n",
    "        #restoreM = tf.squeeze(newM, 2)\n",
    "        # 用归一化除以得到 [batch, seq_len]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的权重对H进行加权求和\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.expand_dims(self.alpha, 2))\n",
    "        output = tf.tanh(tf.squeeze(r, 2))\n",
    "        # 进行dropout处理\n",
    "        output = tf.nn.dropout(output, self.dropout_keep_prob)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def _init_elmo(self):\n",
    "        ## elmo层的输入\n",
    "        self.inputData = tf.placeholder(tf.int32, [None, None], name=\"input_index\")\n",
    "        with tf.variable_scope(\"bilm\", reuse=True):\n",
    "            bilm = BidirectionalLanguageModel(self.config['optionFile'],\n",
    "                                             self.config['weightFile'],\n",
    "                                             use_character_inputs=False,\n",
    "                                             embedding_weight_file=self.config['tokenEmbeddingFile'])\n",
    "            #batcher = TokenBatcher(self.config['vocabFile'])\n",
    "            # 生成batch数据\n",
    "            #inputDataIndex = batcher.batch_sentences(self.inputX)\n",
    "            inputEmbeddingOp = bilm(self.inputData)\n",
    "            self.elmoEmbeddings = weight_layers('input', inputEmbeddingOp, l2_coef=0.0)\n",
    "        \n",
    "    \n",
    "    def init_saver(self):\n",
    "        self.saver = tf.train.Saver(max_to_keep=self.config['max_to_keep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ３. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(BaseTrain):\n",
    "    def __init__(self, sess, model, data, config, logger):\n",
    "        super().__init__(sess, model, data, config, logger)\n",
    "        self.train = data[0]\n",
    "        self.eval = data[1]\n",
    "        \n",
    "        \n",
    "    def train_epoch(self):\n",
    "        num_iter_per_epoch = self.train.length // self.config['batch_size']\n",
    "        for i in tqdm(range(num_iter_per_epoch)):\n",
    "            ## 获取训练结果\n",
    "            loss, metrics, step = self.train_step()\n",
    "            train_acc = metrics['accuracy']\n",
    "            train_f_score = metrics['f_score']\n",
    "            \n",
    "            # 将训练过程的损失写入\n",
    "            summaries_dict = {\"loss\": loss, \n",
    "                             \"acc\": np.array(train_acc),\n",
    "                             \"f_score\": np.array(train_f_score)}\n",
    "            self.logger.summarize(step, summarizer=\"train\", scope=\"train_summary\",\n",
    "                                 summaries_dict=summaries_dict)\n",
    "            if step % self.config['evaluateEvery'] == 0: \n",
    "                print(\"Train —— Step: {} | Loss: {} | Acc: {} : F1_Score: {}\".format(\n",
    "                    step, loss, train_acc, train_f_score))\n",
    "                # 对测试集进行评估\n",
    "                eval_losses = []\n",
    "                eval_pred = []\n",
    "                eval_true = []\n",
    "                for batchEval in self.eval.iter_all(self.config['batch_size']):\n",
    "                    loss, predictions = self.eval_step(batchEval)\n",
    "                    eval_losses.append(loss)\n",
    "                    eval_pred.extend(predictions)\n",
    "                    eval_true.extend(batchEval[-1])\n",
    "                getMetric = Metric(np.array(eval_pred), np.array(eval_true),\n",
    "                                  self.config)\n",
    "                metrics = getMetric.get_metrics()\n",
    "                eval_prec = np.round(metrics['precision'], 5)\n",
    "                eval_recall = np.round(metrics['recall'], 5)\n",
    "                loss_mean = np.round(np.mean(eval_losses), 5)\n",
    "                print(\"Evaluation —— Loss: {} | Precision: {} | Recall: {}\".format(\n",
    "                    loss_mean, eval_prec, eval_recall))\n",
    "                summaries_dict = {\"loss\": np.array(loss_mean),\n",
    "                                 \"precision\": np.array(eval_prec), \n",
    "                                 \"recall\": np.array(eval_recall)}\n",
    "                self.logger.summarize(step, summarizer=\"test\", scope=\"test_summary\",\n",
    "                                     summaries_dict=summaries_dict)\n",
    "            if step % self.config['checkpointEvery'] == 0: \n",
    "                self.model.save(self.sess)\n",
    "            \n",
    "            \n",
    "    def train_step(self):\n",
    "        batch_x,  batch_y = next(self.train.next_batch(self.config['batch_size']))\n",
    "        feed_dict = {self.model.inputX: elmo(batch_x),\n",
    "                     #self.model.inputL: batch_len, \n",
    "                    self.model.inputY: batch_y,\n",
    "                    self.model.dropout_keep_prob: self.config['dropoutProb']}\n",
    "    \n",
    "        _, loss, predictions, step = self.sess.run([self.model.train_op,\n",
    "                                                   self.model.loss,\n",
    "                                                   self.model.predictions, \n",
    "                                                   self.model.global_step_tensor],\n",
    "                                                  feed_dict=feed_dict)\n",
    "        getMetric = Metric(predictions, batch_y, self.config)\n",
    "        metrics = getMetric.get_metrics()\n",
    "        return loss, metrics, step\n",
    "    \n",
    "    def eval_step(self, *batch):\n",
    "        feed_dict = {self.model.inputX: elmo(batch[0]),\n",
    "                     #self.model.inputL: batch[1],\n",
    "                    self.model.inputY: batch[-1],\n",
    "                    self.model.dropout_keep_prob: 1.0}\n",
    "        loss, predictions = self.sess.run([self.model.loss, self.model.predictions],\n",
    "                                         feed_dict=feed_dict)\n",
    "        return loss, predictions\n",
    "    \n",
    "    def elmo(self, reviews):\n",
    "        batcher = TokenBatcher(self.config['vocabFile'])\n",
    "        # 生成batch数据\n",
    "        inputDataIndex = batcher.batch_sentences(reviews)\n",
    "        # 计算ELMO向量表示\n",
    "        elmoVec = self.sess.run(self.model.elmoEmbeddings, \n",
    "                                feed_dict={self.model.inputData: inputDataIndex})[\"weighted_op\"]\n",
    "        return elmoVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 使用数据集训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 使用imdb数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/imdb/labeldTrain.csv\"\n",
    "config = Config(path)\n",
    "data = Dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/bilm-0.1.post5-py3.7.egg/bilm/model.py:384: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "USING SKIP CONNECTIONS\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/bilm-0.1.post5-py3.7.egg/bilm/model.py:524: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/bilm-0.1.post5-py3.7.egg/bilm/model.py:569: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, eval_X, eval_y = np.array(data.trainReviews), np.array(data.trainLabels), np.array(data.evalReviews), np.array(data.evalLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Unable to open object (component not found)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-370237735a34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 创建模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiLSTMAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-15590d3b1ef0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_elmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_saver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-15590d3b1ef0>\u001b[0m in \u001b[0;36m_init_elmo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# 生成batch数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;31m#inputDataIndex = batcher.batch_sentences(self.inputX)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0minputEmbeddingOp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbilm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melmoEmbeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputEmbeddingOp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_coef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bilm-0.1.post5-py3.7.egg/bilm/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, ids_placeholder)\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0membedding_weight_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_weight_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0muse_character_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_character_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                     max_batch_size=self._max_batch_size)\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bilm-0.1.post5-py3.7.egg/bilm/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options, weight_file, ids_placeholder, use_character_inputs, embedding_weight_file, max_batch_size)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bilm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bilm-0.1.post5-py3.7.egg/bilm/model.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_word_char_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_word_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_lstms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bilm-0.1.post5-py3.7.egg/bilm/model.py\u001b[0m in \u001b[0;36m_build_word_embeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    468\u001b[0m             self.embedding_weights = tf.get_variable(\n\u001b[1;32m    469\u001b[0m                 \u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_tokens_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             )\n\u001b[1;32m    472\u001b[0m             self.embedding = tf.nn.embedding_lookup(self.embedding_weights,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1477\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1218\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    528\u001b[0m           function_utils.has_kwargs(custom_getter)):\n\u001b[1;32m    529\u001b[0m         \u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"constraint\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       return _true_getter(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bilm-0.1.post5-py3.7.egg/bilm/model.py\u001b[0m in \u001b[0;36mcustom_getter\u001b[0;34m(getter, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainable'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             kwargs['initializer'] = _pretrained_initializer(\n\u001b[0;32m--> 264\u001b[0;31m                 \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_weight_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             )\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bilm-0.1.post5-py3.7.egg/bilm/model.py\u001b[0m in \u001b[0;36m_pretrained_initializer\u001b[0;34m(varname, weight_file, embedding_weight_file)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_embed_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvarname_in_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;31m# Tensorflow initializers are callables that accept a shape parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0motype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Unable to open object (component not found)'"
     ]
    }
   ],
   "source": [
    "train_data = DataGenerator(train_y, train_X)\n",
    "eval_data = DataGenerator(eval_y, eval_X)\n",
    "pack_data = [train_data, eval_data]\n",
    "\n",
    "## 设置计算图的配置\n",
    "session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "session_conf.gpu_options.allow_growth = True\n",
    "session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9 \n",
    "\n",
    "sess = tf.Session(config=session_conf)\n",
    "\n",
    "# 创建模型\n",
    "model = BiLSTMAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(sess, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(sess, model, pack_data, config, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前正处于第1次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d0220610774ba1a9510b598fad7f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "当前正处于第2次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fb1835f8284465ba41e3e8431b8acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "当前正处于第3次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7757f8d910fe4627a8e3be6e825c3b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "当前正处于第4次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161f7492eaa74bd58347872ba24b8d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "当前正处于第5次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a01ec6d201400a988b562d3b83337e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "当前正处于第6次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a722cf2de484fbaabc7b55daab5f264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "当前正处于第7次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443dfa624b204cc68320875295639ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "当前正处于第8次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e1510fe8f7480798bf86cd768482e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "当前正处于第9次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9579a6dc5c194afc9ac0f7291516c370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "当前正处于第10次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebc9a017ad340c9a694d100b55ba695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
